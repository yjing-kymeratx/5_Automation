{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/fsx/home/yjing/apps/anaconda3/env/yjing/bin python\n",
    "\n",
    "##############################################################################################\n",
    "##################################### load packages ###########################################\n",
    "##############################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import chardet\n",
    "import argparse\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "from d360api import d360api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateToday = datetime.datetime.today().strftime('%Y%b%d')\n",
    "\n",
    "## =====================================================================================   \n",
    "## =================================== load argparses ==================================\n",
    "## =====================================================================================\n",
    "def Step_0_load_args():\n",
    "    parser = argparse.ArgumentParser(description='Usage ot cxcalc_runner.py, details to be added later')\n",
    "    parser.add_argument('-q', action=\"store\", type=int, default=None, help='D360 Query ID')\n",
    "    parser.add_argument('-i', action=\"store\", default=None, help='The input file downloaded from D360')\n",
    "    parser.add_argument('--colName_cid', action=\"store\", default=\"Compound Name\", help='The column name of mol KT ID')\n",
    "    parser.add_argument('--colName_smi', action=\"store\", default=\"Structure\", help='The column name of SMILES')\n",
    "    parser.add_argument('--colName_eid', action=\"store\", default=\"External ID\", help='The column name of external ID')\n",
    "    parser.add_argument('--colName_prj', action=\"store\", default=\"Concat;Project\", help='The column name of Projects')\n",
    "    parser.add_argument('--prop_dict_file', action=\"store\", default=\"prop_cols_matches.json\", help='The json file which specify the property of interest and the columns infomation')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "##--------------------------------------------------------------\n",
    "def folderChecker(my_folder='./my_folder'):\n",
    "    # Check if the folder exists\n",
    "    check_folder = os.path.isdir(my_folder)\n",
    "    # os.path.exists(dir_outputs)\n",
    "    # If the folder does not exist, create it\n",
    "    if not check_folder:\n",
    "        os.makedirs(my_folder)\n",
    "        print(f\"\\tCreated folder:\", my_folder)\n",
    "    else:\n",
    "        print(f'\\t{my_folder} is existing')\n",
    "    return my_folder\n",
    "\n",
    "################################################################################################\n",
    "############################ Step-1. download & load data from D360 ############################\n",
    "################################################################################################\n",
    "def dataDownload(my_query_id=3539, user_name=\"yjing@kymeratx.com\", tokenFile='yjing_D360.token'):\n",
    "    # Create API connection to the PROD server\n",
    "    my_d360 = d360api(provider=\"https://10.3.20.47:8080\")  # PROD environment\n",
    "    user_name = user_name\n",
    "    tokenFile = tokenFile\n",
    "    \n",
    "    with open(tokenFile, 'r') as ofh:\n",
    "        service_token = ofh.readlines()[0]\n",
    "\n",
    "    # Authenticate connection using service token\n",
    "    print(f\"\\tThe D360 query ID is {my_query_id}\")\n",
    "    my_d360.authenticate_servicetoken(servicetoken=service_token, user=user_name)\n",
    "    results = my_d360.download_query_results(query_id=my_query_id)\n",
    "    return results\n",
    "\n",
    "##--------------------------------------------------------------\n",
    "def determine_encoding(dataFile):\n",
    "    # Step 1: Open the CSV file in binary mode\n",
    "    with open(dataFile, 'rb') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Step 2: Detect the encoding using the chardet library\n",
    "    encoding_result = chardet.detect(data)\n",
    "\n",
    "    # Step 3: Retrieve the encoding information\n",
    "    encoding = encoding_result['encoding']\n",
    "\n",
    "    # Step 4: Print/export the detected encoding information\n",
    "    # print(\"Detected Encoding:\", encoding)\n",
    "    return encoding\n",
    "\n",
    "################################################################################################\n",
    "def Step_1_load_data(my_query_id=3539, dataFile=None, tmp_folder=\"./tmp\"):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    ## ------------------------------------------------------------------\n",
    "    assert my_query_id is not None or dataFile is not None, f\"\\tError, both <my_query_id> and <dataFile> are None\"\n",
    "    if my_query_id is not None:\n",
    "        print(f\"\\tRun D360 query on ID {my_query_id}\")\n",
    "        ## download data from D360 using API\n",
    "        dataTableFileName = dataDownload(my_query_id=my_query_id)\n",
    "        print(f'\\tAll data have been downloaded in file {dataTableFileName}')\n",
    "\n",
    "        ## move the csv file to tmp folder\n",
    "        dataFile = f\"{tmp_folder}/{dataTableFileName}\"\n",
    "        shutil.move(dataTableFileName, dataFile)\n",
    "        print(f\"\\tMove the downloaded file {dataTableFileName} to {dataFile}\")\n",
    "    else:\n",
    "        print(f\"\\tDirectly loading data from {dataFile}\")\n",
    "\n",
    "    try:\n",
    "        ## determine encoding type\n",
    "        encoding = determine_encoding(dataFile)\n",
    "        ## read csv file\n",
    "        print(f\"\\tNow reading csv data using <{encoding}> encoding from {dataFile}\")\n",
    "        dataTable = pd.read_csv(dataFile, encoding=encoding).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f'\\tError: cannot read output file {dataFile}; error msg: {e}')\n",
    "        dataTable = None\n",
    "    else:\n",
    "        print(f\"\\tThe loaded raw data has <{dataTable.shape[0]}> rows and {dataTable.shape[1]} columns\")\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"==> The step 1 costs time = %ds ................\" % (costTime))\n",
    "    \n",
    "    return dataTable  \n",
    "\n",
    "################################################################################################\n",
    "###################### Step-2. clean up data and calculate property ############################\n",
    "################################################################################################\n",
    "## ------------------------------------------------------------------\n",
    "def _cleanUpSmiles(smi):\n",
    "    try:\n",
    "        ## text processing\n",
    "        if \"|\" in smi:\n",
    "            smi = smi.split(\"|\")[0]\n",
    "        smi = smi.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"\\r\\n\", \"\")\n",
    "\n",
    "        ## rdkit checking\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        smi_rdkit = Chem.MolToSmiles(mol)\n",
    "    except:\n",
    "        smi_rdkit = np.nan\n",
    "    return smi_rdkit\n",
    "\n",
    "## ------------------------------------------------------------------\n",
    "def CheckThePropertyDataStats(dataTable, col_prop_prefix, propName):\n",
    "    col_mod, col_num = f\"{col_prop_prefix}(Mod)\", f\"{col_prop_prefix}(Num)\"\n",
    "    if (col_mod in dataTable) and (col_num in dataTable):\n",
    "        cond_1 = (dataTable[col_mod]=='=')\n",
    "        cond_2 = (dataTable[col_num].notna())\n",
    "        # print(dataTable[cond_1].shape, dataTable[cond_2].shape)\n",
    "        data_size_available = dataTable[cond_1 & cond_2].shape[0]\n",
    "        print(f\"\\tThere are total <{data_size_available}> existing data for <{propName}>\")\n",
    "        passCheck = True\n",
    "    else:\n",
    "        print(f\"\\tWarning! The column <{col_prop_prefix}(Mod)/(Num)> is not in the table.\")\n",
    "        passCheck = False\n",
    "    return passCheck\n",
    "\n",
    "## ------------------------------------------------------------------\n",
    "def clean_up_prop_data(row, col_prop_prefix, propName):\n",
    "    colName_mod = f\"{col_prop_prefix}(Mod)\"\n",
    "    colName_num = f\"{col_prop_prefix}(Num)\"\n",
    "\n",
    "    if row[colName_mod] == '=' and row.notna()[colName_num]:\n",
    "        result = row[colName_num] \n",
    "    else:\n",
    "        result = np.nan\n",
    "    return result\n",
    "\n",
    "## ----------------------------- F% and EstFa -------------------------------------\n",
    "def rm_elacridar_records(row, col_perctgF='Bioavailability', col_vehicle='ADME PK;Concat;Vehicle'):\n",
    "    result = row[col_perctgF]\n",
    "    if row.notna()[col_vehicle]:\n",
    "        if 'elacridar' in row[col_vehicle]:\n",
    "            result = np.nan\n",
    "            # print(f\"\\t------>change from {row[col_perctgF]} to np.nan, {row[col_vehicle]}\")\n",
    "    return result\n",
    "\n",
    "def calc_EstFa_fromAdm(PKF_PO, Clobs_IV, Species='Rat'):\n",
    "    dict_IV_ratio = {'Rat': 90, 'Mouse': 70, 'Dog': 30, 'Monkey': 44}    \n",
    "    try:\n",
    "        estfa = (PKF_PO/100)/(1-(Clobs_IV/dict_IV_ratio[Species]))\n",
    "    except Exception as e:\n",
    "        estfa = np.nan\n",
    "    return estfa\n",
    "\n",
    "def calc_EstFa(row, colName_pctF, colName_Clobs, Species='Rat'):\n",
    "    try:\n",
    "        pctgF_PO, Clobs_IV = row[colName_pctF], row[colName_Clobs]\n",
    "    except Exception as e:\n",
    "        # print(f\"\\tWarning! Cannot get data for this row from column <{colName_pctgF}> or <{colName_Clobs}>\")\n",
    "        result = np.nan\n",
    "    else:\n",
    "        result = calc_EstFa_fromAdm(pctgF_PO, Clobs_IV, Species=Species)\n",
    "    return result\n",
    "\n",
    "## ----------------------------- hERG -------------------------------------\n",
    "def calc_mean(value_list):\n",
    "    value_list_clean = []\n",
    "    for v in value_list:\n",
    "        if v not in [None, np.nan, '', ' ']:\n",
    "            try:\n",
    "                v_num = float(v)\n",
    "            except Exception as e:\n",
    "                print(f'\\tError, cannot numericalize value {v}', e)\n",
    "            else:\n",
    "                value_list_clean.append(v_num)\n",
    "    return np.mean(value_list_clean)\n",
    "\n",
    "def calc_eIC50_hERG_from_cmt(comments_str):\n",
    "    # e.g., comments_str = '21.38% inhibition @ 10 ?M' or '11.17 inhibition @ 3 ?M'\n",
    "    try:\n",
    "        [str_inhb, str_conc] = comments_str.split('@')\n",
    "\n",
    "        if '%' in str_inhb:\n",
    "            inhb = str_inhb.split('%')[0]\n",
    "        elif 'inhibit' in str_inhb:\n",
    "            inhb = str_inhb.split('inhibit')[0]\n",
    "        else:\n",
    "            inhb = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            inhb = float(inhb)\n",
    "        except:\n",
    "            eIC50 = None\n",
    "        else:\n",
    "            inhb = 0.1 if inhb < 0 else (99.99 if inhb > 100 else inhb)\n",
    "            conc = float(str_conc.split('M')[0][:-1])\n",
    "            eIC50 = conc*(100-inhb)/inhb\n",
    "            \n",
    "    except Exception as e:\n",
    "        eIC50 = None\n",
    "        if comments_str not in [' ', '/']:\n",
    "            print(f'\\tError, cannot calc hERG eIC50 from comment data. {comments_str}')\n",
    "    return eIC50\n",
    "\n",
    "def calc_hERG_eIC50(row, col_hERG_cmts):\n",
    "    if col_hERG_cmts in row:\n",
    "        if row.notna()[col_hERG_cmts]:\n",
    "            hERG_eIC50_list = []\n",
    "            for cmnt in row[col_hERG_cmts].split(';'):\n",
    "                this_eIC50 = calc_eIC50_hERG_from_cmt(cmnt)\n",
    "                hERG_eIC50_list.append(this_eIC50)\n",
    "            result = calc_mean(hERG_eIC50_list)\n",
    "        else:\n",
    "            result = np.nan\n",
    "            # print(f\"\\tNo data in this row for column <{col_hERG_cmts}>\")\n",
    "    else:\n",
    "        result = np.nan\n",
    "        print(f\"\\tColumn <{col_hERG_cmts}> is not in the Table\")\n",
    "    return result\n",
    "\n",
    "def calc_hERG_mIC50(row, col_hERG_IC50, col_hERG_eIC50):\n",
    "    if row.notna()[col_hERG_IC50]:\n",
    "        result = row[col_hERG_IC50]\n",
    "    elif row.notna()[col_hERG_eIC50]:\n",
    "        result = row[col_hERG_eIC50]\n",
    "    else:\n",
    "        result = np.nan\n",
    "    return result\n",
    "\n",
    "################################################################################################\n",
    "def Step_2_clean_data(dataTable, dict_prop_cols, colName_mid, colName_smi, tmp_folder=\"./tmp\"):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    ## ------------------------------------------------------------------\n",
    "    print(f'\\tChecking the vadality of the SMILES using RDKit ...')\n",
    "    dataTable[f\"{colName_smi}_raw\"] = dataTable[colName_smi].apply(lambda x: x)\n",
    "    dataTable[colName_smi] = dataTable[colName_smi].apply(_cleanUpSmiles)\n",
    "    dataTable = dataTable.dropna(subset=[colName_mid, colName_smi]).reset_index(drop=True)\n",
    "    print(f'\\tThere are total <{dataTable.shape[0]}> molecules with valid SMILES<{colName_smi}>')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    for prop in dict_prop_cols:\n",
    "        passCheck = CheckThePropertyDataStats(dataTable, col_prop_prefix=dict_prop_cols[prop], propName=prop)\n",
    "        if passCheck:\n",
    "            dataTable[prop] = dataTable.apply(lambda row: clean_up_prop_data(row, col_prop_prefix=dict_prop_cols[prop], propName=prop), axis=1)\n",
    "\n",
    "        ## remove the 'elacridar' records\n",
    "        if prop == 'Bioavailability':\n",
    "            print(f\"\\t    The num rows with cleaned <{prop}> data (raw) is:\", str(dataTable[dataTable[prop].notna()].shape[0]))\n",
    "            dataTable[prop] = dataTable.apply(lambda row: rm_elacridar_records(row, col_perctgF=prop, col_vehicle='ADME PK;Concat;Vehicle'), axis=1)\n",
    "            print(f\"\\t    The num rows with cleaned <{prop}> data (no elacridar) is:\", str(dataTable[dataTable[prop].notna()].shape[0]))\n",
    "\n",
    "        ## calc estFa\n",
    "        if prop == 'estFa':\n",
    "            dataTable[prop] = dataTable.apply(lambda row: calc_EstFa(row, 'Bioavailability', 'Cl_obs', Species='Rat'), axis=1)\n",
    "\n",
    "        ## calc hERG eIC50\n",
    "        if prop == 'hERG_eIC50':\n",
    "            dataTable[prop] = dataTable.apply(lambda row: calc_hERG_eIC50(row, dict_prop_cols[prop]), axis=1)\n",
    "        \n",
    "        if prop == 'hERG_mixedIC50':\n",
    "            dataTable[prop] = dataTable.apply(lambda row: calc_hERG_mIC50(row, 'hERG_IC50', 'hERG_eIC50'), axis=1)\n",
    "\n",
    "        ## rename MW\n",
    "        if prop == 'MW':\n",
    "            dataTable[prop] = dataTable[dict_prop_cols[prop]].apply(lambda x: x)\n",
    "\n",
    "        ## report\n",
    "        print(f\"\\t    The num rows with cleaned <{prop}> data is:\", str(dataTable[dataTable[prop].notna()].shape[0]))\n",
    "    \n",
    "    ## ------------------------------------------------------------------\n",
    "    colNames_basic = [colName_mid, colName_smi]\n",
    "    colName_props = list(dict_prop_cols.keys())\n",
    "    dataTable_4_mmp = dataTable[colNames_basic + colName_props]\n",
    "\n",
    "    dateToday = datetime.datetime.today().strftime('%Y%b%d')\n",
    "    dataTable_4_mmp.to_csv(f'{tmp_folder}/Data_4_MMP_{dateToday}.csv', index=False)\n",
    "    print(f'\\tThe cleaned dataTable have data shape {dataTable_4_mmp.shape}')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"==> The step 2 costs time = %ds ................\" % (costTime))\n",
    "    return dataTable_4_mmp \n",
    "\n",
    "################################################################################################\n",
    "################################### Step-3. MMPs analysis ######################################\n",
    "################################################################################################\n",
    "## ---------------- prepare the Smiles file and property file ----------------\n",
    "def prep_smi_file(dataTable, colName_prop_list, colName_mid='Compound Name', colName_smi='Structure', output_folder='./results'):\n",
    "    print(f\"\\tNow starting preparing the SMILES file and property CSV file for mmpdb ...\")\n",
    "    \n",
    "    ## the SMILES file for fragmentation\n",
    "    file_smi = f'{output_folder}/Compounds_All.smi'\n",
    "    file_prop_csv = f'{output_folder}/Property_All.csv'\n",
    "    delimiter=' '\n",
    "    ##\n",
    "    data_dict_prop = {}\n",
    "    with open(file_smi, \"w\") as output_file:\n",
    "        # output_file.write(f'SMILES{delimiter_smi}ID' + \"\\n\")\n",
    "        for idx in dataTable.index:\n",
    "            mol_id = dataTable[colName_mid][idx]\n",
    "            mol_smi = dataTable[colName_smi][idx]\n",
    "\n",
    "            ## prepare the SMILES output\n",
    "            this_line = f'{mol_smi}{delimiter}{mol_id}'\n",
    "            output_file.write(this_line + \"\\n\")  # Add a newline character after each string\n",
    "\n",
    "            ## prepare the property CSV output as dict\n",
    "            data_dict_prop[idx] = {}\n",
    "            data_dict_prop[idx]['ID'] = mol_id\n",
    "\n",
    "            for prop_name in colName_prop_list:\n",
    "                try:\n",
    "                    if dataTable[prop_name].notna()[idx]:\n",
    "                        mol_prop = float(dataTable[prop_name][idx])\n",
    "                    else:\n",
    "                        mol_prop = \"*\"\n",
    "                except Exception as e:\n",
    "                    data_dict_prop[idx][prop_name] = \"*\"\n",
    "                    # print(f'\\tThis mol {mol_id} does not have a proper property value: {e}')\n",
    "                else:\n",
    "                    data_dict_prop[idx][prop_name] = mol_prop\n",
    "        \n",
    "    print(f'\\tThe SMILES strings have been saved into .smi file: {file_smi}')\n",
    "        \n",
    "    ## save the csv results\n",
    "    data_table_prop = pd.DataFrame.from_dict(data_dict_prop).T\n",
    "    data_table_prop.to_csv(file_prop_csv, index=False, sep=delimiter)\n",
    "    print(f'\\tThe property data ({data_table_prop.shape}) have been saved into .csv file: {file_smi}')\n",
    "    # data_table_prop.head(3)\n",
    "    return file_smi, file_prop_csv\n",
    "        \n",
    "## ---------------- basic cmd run ----------------\n",
    "def run_cmd(commandLine):\n",
    "    # beginTime = time.time()\n",
    "\n",
    "    # Use subprocess to execute the command\n",
    "    process = subprocess.Popen(commandLine, stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    # costTime = time.time()-beginTime\n",
    "    # print(f\"\\tThis command costs time = %ds ................\" % (costTime))\n",
    "    return (output, error)\n",
    "\n",
    "################################################################################################\n",
    "def Step_3_mmp_analysis(dataTable, dict_prop_cols, colName_mid='Compound Name', colName_smi='Structure', output_folder='./results'):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    ## ------------------------------------------------------------------    \n",
    "    ## prepare the Smiles file and property file\n",
    "    colName_prop_list = list(dict_prop_cols)\n",
    "    file_smi, file_prop_csv = prep_smi_file(dataTable, colName_prop_list, colName_mid, colName_smi, output_folder)\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    ## Fragment the SMILES\n",
    "    file_fragdb = f'{output_folder}/Compounds_All.fragdb'\n",
    "    commandLine_1 = ['mmpdb', 'fragment', file_smi, '-o', file_fragdb]\n",
    "    (output_1, error_1) = run_cmd(commandLine_1)\n",
    "    print(f'\\tThe fragmentation is completed and saved into file {file_fragdb}')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    ## Indexing to find the MMPs in the fragment file & Load the activity/property data\n",
    "    file_mmpdb = f'{output_folder}/Compounds_All.mmpdb'\n",
    "    commandLine_2 = ['mmpdb', 'index', file_fragdb, '-o', file_mmpdb, '--properties', file_prop_csv]\n",
    "    (output_2, error_2) = run_cmd(commandLine_2)\n",
    "    print(f'\\tThe indexing/mmp generation is completed and saved into file {file_mmpdb}')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"==> The step 3 costs time = %ds ................\" % (costTime))\n",
    "\n",
    "    return file_mmpdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "######################################## main ##################################################\n",
    "################################################################################################\n",
    "def main():\n",
    "    ## ------------------------------------------------------------------\n",
    "    print(f\"==> Step 0: load the parameters ... \")\n",
    "    # args = Step_0_load_args()\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    my_query_id = None    # args.q    # 3539\n",
    "    dataFile = './D360_dataset_q_id4975_181224_1833.csv'    # args.i    # None\n",
    "\n",
    "    colName_mid = 'Compound Name'    # args.colName_cid    # 'Compound Name'\n",
    "    colName_smi = 'Structure'    # args.colName_smi    #  or 'Smiles'\n",
    "    # colName_proj = args.colName_prj    # 'Concat;Project'\n",
    "    # colName_eid = args.colName_eid    # 'Concat;External Id'\n",
    "\n",
    "    # Reading JSON data from a file\n",
    "    prop_dict_file = './prop_cols_matches.json'    # args.prop_dict_file\n",
    "    print(prop_dict_file)\n",
    "    with open(prop_dict_file, 'r') as infile:\n",
    "        dict_prop_cols = json.load(infile)\n",
    "    '''\n",
    "    dict_prop_cols = {\n",
    "        'Permeability': 'ADME MDCK(WT) Permeability;Mean;A to B Papp (10^-6 cm/s);', \n",
    "        'Efflux': 'ADME MDCK (MDR1) efflux;Mean;Efflux Ratio;', \n",
    "        'Bioavailability': 'ADME PK;Mean;F %;Dose: 10.000 (mg/kg);Route of Administration: PO;Species: Rat;', \n",
    "        'Cl_obs': 'Copy 1 ;ADME PK;Mean;Cl_obs(mL/min/kg);Dose: 2.000 (mg/kg);Route of Administration: IV;Species: Rat;',\n",
    "        'hERG_IC50': 'ADME Tox-manual patch hERG 34C;GMean;m-patch hERG IC50 [uM];',\n",
    "        'hERG_eIC50': 'ADME Tox-manual patch hERG 34C;Concat;Comments',\n",
    "        'hERG_mixedIC50': 'Not Availale',\n",
    "        'estFa': 'Not Availale',\n",
    "        'MW': 'Molecular Weight',\n",
    "        'bpKa1': 'in Silico PhysChem Property;Mean;Corr_ChemAxon_bpKa1;',\n",
    "        'logD': 'in Silico PhysChem Property;Mean;Kymera ClogD (v1);', \n",
    "        }    \n",
    "    '''\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    ## create tmp folder\n",
    "    tmp_folder = folderChecker(f\"./tmp\")\n",
    "    output_folder = folderChecker(f\"./results\")\n",
    "    \n",
    "    ## ------------------------------------------------------------------\n",
    "    #### Step-1. download & load data from D360\n",
    "    print(f\"==> Step 1: download & load data from D360 ...\")\n",
    "    dataTable = Step_1_load_data(my_query_id, dataFile, tmp_folder)\n",
    "    # dataTable = pd.read_csv(f\"./tmp/D360_dataset_q_id3539_111224_0120.csv\").reset_index(drop=True)\n",
    "    # dataTable.head(3)\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    #### Step-2. clean up data and calculate property\n",
    "    print(f\"==> Step 2: clean up data and calculate new property ...\")\n",
    "    dataTable_4_mmp = Step_2_clean_data(dataTable, dict_prop_cols, colName_mid, colName_smi, tmp_folder)\n",
    "    # dataTable_4_mmp.head(3)\n",
    "    \n",
    "    ## ------------------------------------------------------------------\n",
    "    #### Step-3. MMPs analysis\n",
    "    print(f\"==> Step 3: run MMP analysis using mmpdb ...\")\n",
    "    file_mmpdb = Step_3_mmp_analysis(dataTable_4_mmp, dict_prop_cols, colName_mid, colName_smi, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Step 0: load the parameters ... \n",
      "./prop_cols_matches.json\n",
      "\t./tmp is existing\n",
      "\t./results is existing\n",
      "==> Step 1: download & load data from D360 ...\n",
      "\tDirectly loading data from ./D360_dataset_q_id4975_181224_1833.csv\n",
      "\tNow reading csv data using <utf-8> encoding from ./D360_dataset_q_id4975_181224_1833.csv\n",
      "\tThe loaded raw data has <33694> rows and 46 columns\n",
      "==> The step 1 costs time = 10s ................\n",
      "==> Step 2: clean up data and calculate new property ...\n",
      "\tChecking the vadality of the SMILES using RDKit ...\n",
      "\tThere are total <33693> molecules with valid SMILES<Structure>\n",
      "\tThere are total <3336> existing data for <Permeability>\n",
      "\t    The num rows with cleaned <Permeability> data is: 3336\n",
      "\tThere are total <2102> existing data for <Efflux>\n",
      "\t    The num rows with cleaned <Efflux> data is: 2102\n",
      "\tThere are total <3188> existing data for <Bioavailability>\n",
      "\t    The num rows with cleaned <Bioavailability> data (raw) is: 3188\n",
      "\t    The num rows with cleaned <Bioavailability> data (no elacridar) is: 3188\n",
      "\t    The num rows with cleaned <Bioavailability> data is: 3188\n",
      "\tThere are total <3971> existing data for <Cl_obs>\n",
      "\t    The num rows with cleaned <Cl_obs> data is: 3971\n",
      "\tThere are total <809> existing data for <hERG_IC50>\n",
      "\t    The num rows with cleaned <hERG_IC50> data is: 809\n",
      "\tWarning! The column <ADME Tox-manual patch hERG 34C;Concat;Comments(Mod)/(Num)> is not in the table.\n",
      "\t    The num rows with cleaned <hERG_eIC50> data is: 2176\n",
      "\tWarning! The column <hERG_mixedIC50 Not Availale(Mod)/(Num)> is not in the table.\n",
      "\t    The num rows with cleaned <hERG_mixedIC50> data is: 2980\n",
      "\tWarning! The column <estFa Not Availale(Mod)/(Num)> is not in the table.\n",
      "\t    The num rows with cleaned <estFa> data is: 3161\n",
      "\tWarning! The column <Molecular Weight(Mod)/(Num)> is not in the table.\n",
      "\t    The num rows with cleaned <MW> data is: 33693\n",
      "\tThere are total <33442> existing data for <bpKa1>\n",
      "\t    The num rows with cleaned <bpKa1> data is: 33442\n",
      "\tThere are total <33548> existing data for <logD>\n",
      "\t    The num rows with cleaned <logD> data is: 33548\n",
      "\tThe cleaned dataTable have data shape (33693, 13)\n",
      "==> The step 2 costs time = 25s ................\n",
      "==> Step 3: run MMP analysis using mmpdb ...\n",
      "\tNow starting preparing the SMILES file and property CSV file for mmpdb ...\n",
      "\tThe SMILES strings have been saved into .smi file: ./results/Compounds_All.smi\n",
      "\tThe property data ((33693, 12)) have been saved into .csv file: ./results/Compounds_All.smi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThe fragmentation is completed and saved into file ./results/Compounds_All.fragdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThe indexing/mmp generation is completed and saved into file ./results/Compounds_All.mmpdb\n",
      "==> The step 3 costs time = 2744s ................\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name             #cmpds #rules  #pairs  #envs   #stats   |---------------- Title -----------------| Properties\n",
      "./results/Compounds_All.mmpdb  28311 523402 4354260 3513261 11244761  MMPs from './results/Compounds_All.fragdb' Permeability Efflux Bioavailability Cl_obs hERG_IC50 hERG_eIC50 hERG_mixedIC50 estFa MW bpKa1 logD\n",
      "      Created: 2025-01-03 00:27:53.507548\n",
      "        #compounds/property:  3065/Bioavailability 3811/Cl_obs 2000/Efflux 28311/MW 3143/Permeability 28120/bpKa1 3038/estFa 784/hERG_IC50 2101/hERG_eIC50 2880/hERG_mixedIC50 28205/logD\n",
      "        #smiles for rules: 35932  for constants: 54850\n",
      "        Fragment options:\n",
      "          cut_smarts: [#6+0;!$(*=,#[!#6])]!@!=!#[!#0;!#1;!$([CH2]);!$([CH3][CH2])]\n",
      "          max_heavies: 100\n",
      "          max_rotatable_bonds: 30\n",
      "          max_up_enumerations: 1000\n",
      "          method: chiral\n",
      "          min_heavies_per_const_frag: 0\n",
      "          min_heavies_total_const_frag: 0\n",
      "          num_cuts: 3\n",
      "          rotatable_smarts: [!$([NH]!@C(=O))&!D1&!$(*#*)]-&!@[!$([NH]!@C(=O))&!D1&!$(*#*)]\n",
      "          salt_remover: <default>\n",
      "        Index options:\n",
      "          max_radius: 5\n",
      "          max_variable_heavies: 10\n",
      "          min_radius: 0\n",
      "          smallest_transformation_only: False\n",
      "          symmetric: False\n"
     ]
    }
   ],
   "source": [
    "!mmpdb list --all ./results/Compounds_All.mmpdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_DB_Options(db_location=\"./results/Compounds_All.mmpdb\"):\n",
    "    from playhouse import db_url as playhouse_db_url\n",
    "    db_class = playhouse_db_url.schemes[\"sqlite\"]\n",
    "    db = db_class(database=db_location)\n",
    "\n",
    "    from mmpdblib import schema\n",
    "    MMPsDB = schema.MMPDatabase(db)\n",
    "    dataset = MMPsDB.get_dataset(cursor=None)\n",
    "\n",
    "    cursor = dataset.get_cursor()\n",
    "    fragment_options = dataset.get_fragment_options(cursor)\n",
    "    print(fragment_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FragmentOptions(max_heavies=100, max_rotatable_bonds=30, rotatable_smarts='[!$([NH]!@C(=O))&!D1&!$(*#*)]-&!@[!$([NH]!@C(=O))&!D1&!$(*#*)]', cut_smarts='[#6+0;!$(*=,#[!#6])]!@!=!#[!#0;!#1;!$([CH2]);!$([CH3][CH2])]', num_cuts=3, method='chiral', salt_remover='<default>', min_heavies_per_const_frag=0, max_up_enumerations=1000, min_heavies_total_const_frag=0)\n"
     ]
    }
   ],
   "source": [
    "check_DB_Options(db_location=\"./results/Compounds_All.mmpdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mmpdb fragment ./results/Compounds_All.smi -o ./results/Compounds_All_3.fragdb\n",
    "# !mmpdb index ./results/Compounds_All_3.fragdb -o ./results/Compounds_All_3.mmpdb --properties ./results/Property_All.csv\n",
    "# !mmpdb list --all ./results/Compounds_All_3.mmpdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
