{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "############################### Loading data from database ###################################\n",
    "##############################################################################################\n",
    "def _call_my_query(db_file, my_query):\n",
    "    import sqlite3\n",
    "    \n",
    "    ## connect to the SQLIte database\n",
    "    my_connection = sqlite3.connect(db_file)\n",
    "\n",
    "    ## create a cursor object\n",
    "    my_cursor = my_connection.cursor()\n",
    "\n",
    "    ## excute the query\n",
    "    my_cursor.execute(my_query)\n",
    "\n",
    "    ## fetch all the rows\n",
    "    rows = my_cursor.fetchall()\n",
    "    \n",
    "    # ## export the results\n",
    "    # data_list = [row for row in rows]\n",
    "    my_connection.close()\n",
    "    return rows\n",
    "\n",
    "## ------------- extract table data from SQLite DB ------------- \n",
    "def _extract_tables(db_file, table_name):\n",
    "    ## get header info\n",
    "    my_query_colName = f\"PRAGMA table_info({table_name})\"\n",
    "    column_details = _call_my_query(db_file, my_query_colName)\n",
    "    colName_list = [column[1] for column in column_details]\n",
    "\n",
    "    ## get data info\n",
    "    my_query_data = f\"SELECT * FROM {table_name}\"\n",
    "    data_rows = _call_my_query(db_file, my_query_data)\n",
    "    \n",
    "    return colName_list, data_rows\n",
    "\n",
    "def _write_2_csv(colName_list, data_rows, csv_file_name, delimiter=','):\n",
    "    import csv\n",
    "    with open(csv_file_name, 'w', newline='') as csvfh:\n",
    "        writer = csv.writer(csvfh)    # , delimiter=delimiter\n",
    "        ## --------- Write header ---------\n",
    "        writer.writerow(colName_list)\n",
    "\n",
    "        ## --------- Write data ---------\n",
    "        print(f\"\\tNow start writing the data into csv\")\n",
    "        for i in range(0, len(data_rows)):\n",
    "            writer.writerow(list(data_rows[i]))\n",
    "            if i % 10**6 == 0:\n",
    "                print(f\"\\t\\trow-{i}\")\n",
    "    print(f\"\\tNow the table data were saved into <{csv_file_name}>\")\n",
    "    return None\n",
    "\n",
    "################################################################################################\n",
    "def Step_4_extract_data_from_DB(file_mmpdb, tmp_folder):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    print(f\"4. Now extracting tables from MMPs database ...\")\n",
    "    ## ------------------------------------------------------------------\n",
    "    dataDict_csvFiles = {}\n",
    "    for table_name in [\"pair\", \"compound\", \"compound_property\", \"property_name\", \"constant_smiles\",\n",
    "                    \"rule\", \"rule_smiles\", \"rule_environment\", \"rule_environment_statistics\", \"environment_fingerprint\"]:\n",
    "        \n",
    "        print(f\"\\tNow processing the table <{table_name}>\")\n",
    "        colName_list, data_rows = _extract_tables(file_mmpdb, table_name)       \n",
    "\n",
    "        ## --------- write output ---------\n",
    "        ## define folder and csv fileName\n",
    "        subFolderDB = folderChecker(f\"{tmp_folder}/DB_tables\")\n",
    "        table_out = f\"{subFolderDB}/DB_table_{table_name}.csv\"\n",
    "        ## write 2 csv\n",
    "        _write_2_csv(colName_list, data_rows, table_out)\n",
    "\n",
    "        print(f\"\\t<{table_name}> table has been saved into {table_out}\\n\")\n",
    "        dataDict_csvFiles[table_name] = table_out\n",
    "        # print(table_name)\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"==> Step 4 <Extracting data from MMPs DB> complete, costs time = %ds ................\\n\" % (costTime))    \n",
    "    return dataDict_csvFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mmpdb = '../results/Compounds_All.mmpdb'\n",
    "tmp_folder = folderChecker('./tmp')\n",
    "# dataDict_csvFiles = Step_4_extract_data_from_DB(file_mmpdb, tmp_folder, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict_csvFiles = {'pair': './tmp/DB_tables/DB_table_pair.csv',\n",
    " 'compound': './tmp/DB_tables/DB_table_compound.csv',\n",
    " 'compound_property': './tmp/DB_tables/DB_table_compound_property.csv',\n",
    " 'property_name': './tmp/DB_tables/DB_table_property_name.csv',\n",
    " 'constant_smiles': './tmp/DB_tables/DB_table_constant_smiles.csv',\n",
    " 'rule': './tmp/DB_tables/DB_table_rule.csv',\n",
    " 'rule_smiles': './tmp/DB_tables/DB_table_rule_smiles.csv',\n",
    " 'rule_environment': './tmp/DB_tables/DB_table_rule_environment.csv',\n",
    " 'rule_environment_statistics': './tmp/DB_tables/DB_table_rule_environment_statistics.csv',\n",
    " 'environment_fingerprint': './tmp/DB_tables/DB_table_environment_fingerprint.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------------------------\n",
    "def load_db_table(dataDict_csvFiles, db_table_name, usecols=None, rename_cols={}):\n",
    "    ## find file\n",
    "    db_table_file = dataDict_csvFiles[db_table_name]\n",
    "    ## load data\n",
    "    if usecols is None:\n",
    "        dataTable = pd.read_csv(db_table_file)\n",
    "    else:\n",
    "        dataTable = pd.read_csv(db_table_file, usecols=usecols)\n",
    "    ## clean up table\n",
    "    dataTable.rename(columns=rename_cols, inplace=True)\n",
    "    print(f\"\\tIn the <{db_table_name}> table (selected), there are total {dataTable.shape[0]} rows and {dataTable.shape[1]} cols\")\n",
    "    return dataTable\n",
    "\n",
    "## ----------------------------------------\n",
    "def merge_cmpd_data(df_left, df_right, left_on=['compound1_id', 'compound2_id'], right_on='compound_id', how='left'):\n",
    "    ## from mol\n",
    "    df_left = df_left.merge(right=df_right, left_on=left_on[0], right_on=right_on, how=how)\n",
    "    df_left.rename(columns={col: f'From_{col}' for col in df_right.columns}, inplace=True)\n",
    "    ## to mol\n",
    "    df_left = df_left.merge(right=df_right, left_on=left_on[1], right_on=right_on, how=how)\n",
    "    df_left.rename(columns={col: f'To_{col}' for col in df_right.columns}, inplace=True)\n",
    "    ## clean up columns\n",
    "    df_left.drop(columns=[f'From_{right_on}', f'To_{right_on}'], inplace=True)\n",
    "    print(f\"\\tThe current table has shape <{df_left.shape}>\")\n",
    "    return df_left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load the info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------\n",
    "## ----------- load the prop value table -----------\n",
    "## -------------------------------------------------------\n",
    "dataTable_prop = load_db_table(dataDict_csvFiles, db_table_name=\"compound_property\")\n",
    "\n",
    "##\n",
    "dataDict_prop = {}\n",
    "for idx in dataTable_prop.index:\n",
    "    cmpd_id = dataTable_prop['compound_id'][idx]\n",
    "    prop_id = dataTable_prop['property_name_id'][idx]\n",
    "    prop_value = dataTable_prop['value'][idx]\n",
    "\n",
    "    if cmpd_id not in dataDict_prop:\n",
    "        dataDict_prop[cmpd_id] = {}\n",
    "    if prop_id not in dataDict_prop[cmpd_id]:\n",
    "        dataDict_prop[cmpd_id][prop_id] = round(prop_value, 2)\n",
    "\n",
    "dataTable_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_propName = load_db_table(dataDict_csvFiles, db_table_name=\"property_name\", rename_cols={'id': 'property_name_id'})\n",
    "dataTable_propName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------\n",
    "## ----------- Load the compound table -----------\n",
    "## -------------------------------------------------------\n",
    "dataTable_cmpd = load_db_table(dataDict_csvFiles, \n",
    "                               db_table_name=\"compound\", \n",
    "                               usecols=[\"id\", \"public_id\", \"clean_smiles\"], \n",
    "                               rename_cols={'id': 'compound_id', 'public_id': 'mol_id', 'clean_smiles': 'Structure'})\n",
    "\n",
    "# ## -------------------------------------------------------\n",
    "# ## ----------- load the prop name table -----------\n",
    "# ## -------------------------------------------------------\n",
    "# dataTable_propName = load_db_table(dataDict_csvFiles, db_table_name=\"property_name\", rename_cols={'id': 'property_name_id'})\n",
    "\n",
    "# ## df to dict\n",
    "# dataDict_propName = dict(zip(dataTable_propName['property_name_id'], dataTable_propName['name']))\n",
    "# # print(dataDict_propName)\n",
    "\n",
    "## -------------------------------------------------------\n",
    "## ----------- load the prop value table -----------\n",
    "## -------------------------------------------------------\n",
    "dataTable_prop = load_db_table(dataDict_csvFiles, db_table_name=\"compound_property\", rename_cols={'id': 'prop_id'})\n",
    "\n",
    "# ## ----------- Pivot the prop value table & replace prop name id to prop name-----------\n",
    "# dataTable_prop_pivot = dataTable_prop.pivot(index='compound_id', columns='property_name_id', values='value')\n",
    "# dataTable_prop_pivot.rename(columns=dataDict_propName, inplace=True)\n",
    "# dataTable_prop_pivot.reset_index(names='compound_id', inplace=True)\n",
    "\n",
    "# ## -------------------------------------------------------\n",
    "# ## ----------- load the constant table -----------\n",
    "# ## -------------------------------------------------------\n",
    "\n",
    "dataTable_constsmi = load_db_table(dataDict_csvFiles,\n",
    "                                   db_table_name=\"constant_smiles\", \n",
    "                                   rename_cols={'id': 'constant_id', 'smiles': 'constant_smi'})\n",
    "\n",
    "## -------------------------------------------------------\n",
    "## ----------- load the pair table -----------\n",
    "## -------------------------------------------------------\n",
    "dataTable_pair = load_db_table(dataDict_csvFiles,\n",
    "                               db_table_name=\"pair\", \n",
    "                               rename_cols={'id': 'pair_id'})\n",
    "\n",
    "## -------------------------------------------------------\n",
    "## ---------------- load the rule data ----------------\n",
    "## -------------------------------------------------------\n",
    "## rule table\n",
    "dataTable_rule = load_db_table(dataDict_csvFiles, db_table_name=\"rule\", rename_cols={\"id\": \"rule_id\"})\n",
    "\n",
    "## rule smiles\n",
    "dataTable_rule_smi = load_db_table(dataDict_csvFiles, db_table_name=\"rule_smiles\", usecols=['id', 'smiles'], rename_cols={'id': 'rule_smiles_id'})\n",
    "\n",
    "## -------------------------------------------------------\n",
    "## ----------- load the rule env data -----------\n",
    "## -------------------------------------------------------\n",
    "## rule env table\n",
    "dataTable_rule_env = load_db_table(dataDict_csvFiles,\n",
    "                                   db_table_name=\"rule_environment\", \n",
    "                                   rename_cols={'id': 'rule_environment_id', 'num_pairs': 'rule_env_num_pairs', 'radius': 'rule_env_radius'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_pair['pair_info'] = dataTable_pair['compound1_id'].astype(str) + '_' + dataTable_pair['compound2_id'].astype(str)\n",
    "dataTable_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataTable_pair['pair_info'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame A\n",
    "data = {'ID': [1, 2, 3]}\n",
    "df_A = pd.DataFrame(data)\n",
    "\n",
    "# Define the unique values for the 'type' column\n",
    "type_values = ['Type1', 'Type2', 'Type3', 'Type4', 'Type5']\n",
    "\n",
    "# Repeat each ID 5 times\n",
    "df_expanded = df_A.loc[df_A.index.repeat(len(type_values))].reset_index(drop=True)\n",
    "\n",
    "# Add the 'type' column by repeating the type_values for each unique ID\n",
    "df_expanded['type'] = type_values * len(df_A)\n",
    "\n",
    "print(df_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_pair = dataTable_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# ## ----------- add the compound info into pair table -----------\n",
    "# dataTable_pair = merge_cmpd_data(df_left=dataTable_pair, df_right=dataTable_cmpd, left_on=['compound1_id', 'compound2_id'], right_on='cmpd_id')\n",
    "\n",
    "# ## ----------- add the property data into pair table -----------\n",
    "# dataTable_pair = merge_cmpd_data(df_left=dataTable_pair, df_right=dataTable_prop_pivot, left_on=['compound1_id', 'compound2_id'], right_on='cmpd_id')\n",
    "\n",
    "# ## ----------- add the constant smiles data into pair table -----------\n",
    "# dataTable_pair = dataTable_pair.merge(right=dataTable_constsmi, on='constant_id', how='left')\n",
    "\n",
    "# ## clean up dable\n",
    "# dataTable_pair.drop(columns=[\"compound1_id\", \"compound2_id\", \"constant_id\"], inplace=True)\n",
    "dataTable_pair.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_merged = pd.merge(left=dataTable_pair, right=dataTable_rule_env, on=\"rule_environment_id\", how=\"left\")\n",
    "dataTable_merged = pd.merge(left=dataTable_merged, right=dataTable_rule, on=\"rule_id\", how=\"left\")\n",
    "# dataTable_merged['Pair_idx'] = dataTable_merged['compound1_id'].astype(str) + '_' + dataTable_merged['compound2_id'].astype(str)\n",
    "# dataTable_merged['Pair_rule_idx'] = dataTable_merged['Pair_idx'] + '_' + dataTable_merged['rule_id'].astype(str)\n",
    "# 'Pair_idx', 'Pair_rule_idx', \n",
    "dataTable_merged = dataTable_merged[[\"pair_id\", \"compound1_id\", \"compound2_id\", \"rule_id\", \"from_smiles_id\", \"to_smiles_id\", \"constant_id\", \"rule_environment_id\", \"rule_env_radius\", \"rule_env_num_pairs\", \"environment_fingerprint_id\"]]\n",
    "print(dataTable_merged.shape)\n",
    "dataTable_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataTable_merged = pd.merge(left=dataTable_merged, right=dataTable_prop, left_on=\"compound1_id\", right_on=\"compound_id\", how=\"left\")\n",
    "# print(dataTable_merged.shape)\n",
    "# dataTable_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataTable_merged = pd.merge(left=dataTable_merged, right=dataTable_prop, left_on=\"compound2_id\", right_on=\"compound_id\", how=\"left\")\n",
    "# print(dataTable_merged.shape)\n",
    "# dataTable_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule_environment_id\tproperty_name_id\tcount\tavg\tstd\tmin\tmedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prop_id\tcompound_id\tproperty_name_id\tvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ----------------------------------------\n",
    "# def merge_cmpd_data(df_left, df_right, left_on=['compound1_id', 'compound2_id'], right_on='compound_id', how='left'):\n",
    "#     ## from mol\n",
    "#     df_left = df_left.merge(right=df_right, left_on=left_on[0], right_on=right_on, how=how)\n",
    "#     df_left.rename(columns={col: f'From_{col}' for col in df_right.columns}, inplace=True)\n",
    "#     df_left.drop(columns=[f'From_{right_on}'], inplace=True)\n",
    "    \n",
    "#     ## to mol\n",
    "#     df_left = df_left.merge(right=df_right, left_on=left_on[1], right_on=right_on, how=how)\n",
    "#     df_left.rename(columns={col: f'To_{col}' for col in df_right.columns}, inplace=True)\n",
    "#     df_left.drop(columns=[f'To_{right_on}'], inplace=True)\n",
    "    \n",
    "#     print(f\"\\tThe current table has shape <{df_left.shape}>\")\n",
    "#     return df_left\n",
    "\n",
    "# dataTable_merged = merge_cmpd_data(df_left=dataTable_merged, df_right=dataTable_prop, left_on=['compound1_id', 'compound2_id'], right_on='compound_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"KT_number_1\", \"smiles_1\", \n",
    "\"KT_number_2\", \"smiles_2\", \n",
    "\n",
    "\"property_name\", \"property_values_1\", \"property_values_2\"\n",
    "\n",
    "\n",
    "\"rule_id\"\n",
    "\"rule_from_smiles\"\n",
    "\"rule_to_smiles\"\n",
    "\"rule_constant_smiles\"\n",
    "\"rule_Transformation\" Position\n",
    "\"rule_env_avg\"\n",
    "\"rule_env_count\"\n",
    "\"rule_env_stats\"\n",
    "\"rule_comments\"\n",
    "\n",
    "[\"smiles_1\", \"External_ID_1\", \"External_ID_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"rule_environment_id\": {\"radius\": 1, \"num_pairs\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------\n",
    "## ---------------- load the rule data ----------------\n",
    "## -------------------------------------------------------\n",
    "## rule table\n",
    "dataTable_rule = load_db_table(dataDict_csvFiles, db_table_name=\"rule\", rename_cols={\"id\": \"rule_id\"})\n",
    "\n",
    "## rule smiles\n",
    "dataTable_rule_smi = load_db_table(dataDict_csvFiles, db_table_name=\"rule_smiles\", usecols=['id', 'smiles'], rename_cols={'id': 'rule_smiles_id'})\n",
    "\n",
    "## ----------- add the rule smiles info into rule table -----------\n",
    "dataTable_rule = merge_cmpd_data(df_left=dataTable_rule, df_right=dataTable_rule_smi, left_on=['from_smiles_id', 'to_smiles_id'], right_on='rule_smiles_id')\n",
    "dataTable_rule.drop(columns=[\"from_smiles_id\", \"to_smiles_id\"], inplace=True)\n",
    "\n",
    "## -------------------------------------------------------\n",
    "## ----------- load the rule env data -----------\n",
    "## -------------------------------------------------------\n",
    "## rule env table\n",
    "dataTable_rule_env = load_db_table(dataDict_csvFiles,\n",
    "                                   db_table_name=\"rule_environment\", \n",
    "                                   rename_cols={'id': 'rule_environment_id', 'num_pairs': 'rule_env_num_pairs', 'radius': 'rule_env_radius'})\n",
    "\n",
    "## ----------- load the rule env stats data -----------\n",
    "dataTable_rule_env_stats = load_db_table(dataDict_csvFiles,\n",
    "                                         db_table_name=\"rule_environment_statistics\",\n",
    "                                         usecols=['rule_environment_id', 'property_name_id', 'count', 'avg', 'std', 'min', 'median', 'median'],\n",
    "                                         rename_cols={})\n",
    "\n",
    "#######################################################################################################\n",
    "## ----------- merge rule env info -----------\n",
    "dataTable_rule_env = dataTable_rule_env.merge(right=dataTable_rule, on=\"rule_id\", how=\"left\")\n",
    "dataTable_rule_env = dataTable_rule_env.merge(right=dataTable_rule_env_stats, on=\"rule_environment_id\", how=\"left\")\n",
    "dataTable_rule_env['property_name'] = dataTable_rule_env['property_name_id'].apply(lambda x: dataDict_propName[x])\n",
    "dataTable_rule_env.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_rule_env.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_rule_env['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------- load the rule env stats data -----------\n",
    "dataTable_rule_env_stats = load_db_table(dataDict_csvFiles,\n",
    "                                         db_table_name=\"rule_environment_statistics\",\n",
    "                                         usecols=['rule_environment_id', 'property_name_id', 'count', 'avg', 'std', 'min', 'median', 'median'],\n",
    "                                         rename_cols={})\n",
    "dataTable_rule_env_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'id', 'environment_fingerprint_id'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Step_5_MMPs_DataClean(dataDict_tables, add_symetric=True):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    print(f\"5. Now clean up the MMPs data ...\")\n",
    "    ## ------------------------------------------------------------------\n",
    "\n",
    "    ## get the individual database Tables\n",
    "    dataTable_pair = dataDict_tables[\"pair\"]\n",
    "    dbTable_cmpd = dataDict_tables[\"compound\"]\n",
    "    dbTable_propName = dataDict_tables[\"property_name\"]\n",
    "    dbTable_propValue = dataDict_tables[\"compound_property\"]\n",
    "\n",
    "    ## ------------- build the dataDict of pairs -------------\n",
    "    print(f\"\\tNow start cleanning up the dataDict of pairs ...\\n\")\n",
    "    dataDict = {}\n",
    "    for idx in dataTable_pair.index:\n",
    "        pair_idx = dataTable_pair['id'][idx]\n",
    "        cid_1 = int(dataTable_pair['compound1_id'][idx])\n",
    "        cid_2 = int(dataTable_pair['compound2_id'][idx])\n",
    "        const_id = dataTable_pair['constant_id'][idx]\n",
    "        rule_env_id = dataTable_pair['rule_environment_id'][idx]\n",
    "\n",
    "        ## initialize the sub-dict\n",
    "        pair_info = f\"{cid_1}==>{cid_2}\"\n",
    "        try:\n",
    "            pair_list = sorted([cid_1, cid_2], reverse=False)\n",
    "        except Exception as e:\n",
    "            pass    \n",
    "        \n",
    "        if pair_info not in dataDict:\n",
    "            ## add pair basic info\n",
    "            dataDict[pair_info] = {}\n",
    "            dataDict[pair_info][\"pair_info\"] = pair_info\n",
    "            dataDict[pair_info][\"pair_id\"] = f\"({min([cid_1, cid_2])},{max([cid_1, cid_2])})\"\n",
    "            dataDict[pair_info][\"compound1_id\"] = cid_1\n",
    "            dataDict[pair_info][\"compound2_id\"] = cid_2\n",
    "            dataDict[pair_info][\"pair_detail\"] = {}\n",
    "\n",
    "            ## add compound info\n",
    "            dataDict[pair_info][\"From_mol_id\"] = dbTable_cmpd['public_id'][cid_1]\n",
    "            dataDict[pair_info][\"To_mol_id\"] = dbTable_cmpd['public_id'][cid_2]\n",
    "            smi_1, smi_2 = dbTable_cmpd['input_smiles'][cid_1], dbTable_cmpd['input_smiles'][cid_2]\n",
    "            dataDict[pair_info][\"From_Structure\"] = smi_1\n",
    "            dataDict[pair_info][\"To_Structure\"] = smi_2\n",
    "            \n",
    "            ## add shared structure\n",
    "            # dataDict[pair_info][\"SharedSubstructure\"] = fun_tbd(smi_1, smi_2)\n",
    "\n",
    "            ## add compound prop info\n",
    "            for prop_id in dbTable_propName.index:\n",
    "                prop_name = dbTable_propName['name'][prop_id]\n",
    "                dataDict[pair_info][f\"From_{prop_name}\"] = _findPropValue(dbTable_propValue, cid_1, prop_id, average=True)\n",
    "                dataDict[pair_info][f\"To_{prop_name}\"] = _findPropValue(dbTable_propValue, cid_2, prop_id, average=True)\n",
    "                ## add delta value change\n",
    "                try:\n",
    "                    delta_value = dataDict[pair_info][f\"To_{prop_name}\"] - dataDict[pair_info][f\"From_{prop_name}\"]\n",
    "                except Exception as e:\n",
    "                    delta_value = np.nan\n",
    "                    \n",
    "                dataDict[pair_info][f\"Delta_{prop_name}\"] = delta_value\n",
    "\n",
    "        ## add pair details information (constant part)\n",
    "        if const_id not in dataDict[pair_info][\"pair_detail\"]:\n",
    "            dataDict[pair_info][\"pair_detail\"][const_id] = []\n",
    "        \n",
    "        ## add pair details information (rule_env)\n",
    "        if rule_env_id not in dataDict[pair_info][\"pair_detail\"][const_id]:\n",
    "            dataDict[pair_info][\"pair_detail\"][const_id].append(rule_env_id)\n",
    "    print(f\"\\t\\tOriginal num_pairs in dataDict: {len(dataDict)}\\n\")\n",
    "    \n",
    "    ## ------------- add the symetric pairs if not exist -------------\n",
    "    tran_smi = True\n",
    "    radius = 0    # [0, 1, 2, 3, 4, 5]\n",
    "    sele_rule = \"max\"    # [\"max\", \"min\"]\n",
    "    if tran_smi:\n",
    "        for pair_info in dataDict:\n",
    "            pair_detail = dataDict[pair_info][\"pair_detail\"]\n",
    "            const_smi_sele, from_smiles, to_smiles = _findTranSmi(pair_detail, dataDict_tables, sele_rule=sele_rule, radius=radius)\n",
    "            dataDict[pair_info][\"constant_smiles\"] = const_smi_sele\n",
    "            dataDict[pair_info][\"from_smiles\"] = from_smiles\n",
    "            dataDict[pair_info][\"to_smiles\"] = to_smiles\n",
    "        \n",
    "    ## ------------- add the symetric pairs if not exist -------------\n",
    "    if add_symetric:\n",
    "        print(f\"\\t\\tNow adding symetric pairs ...\")\n",
    "        list_pair_info_4loop = copy.deepcopy(list(dataDict.keys()))\n",
    "        list_pair_info_4check = copy.deepcopy(list(dataDict.keys()))\n",
    "        for pair_info in list_pair_info_4loop:\n",
    "            if pair_info in list_pair_info_4check:\n",
    "                list_pair_info_4check.remove(pair_info)\n",
    "\n",
    "                ## reverse pair\n",
    "                cid_1, cid_2 = pair_info.split(\"==>\")\n",
    "                pair_info_revs = f\"{cid_2}==>{cid_1}\"\n",
    "                if pair_info_revs in list_pair_info_4check:\n",
    "                    list_pair_info_4check.remove(pair_info_revs)\n",
    "                else:\n",
    "                    ## if reversed pair not in check list, add it in the dict\n",
    "                    dataDict[pair_info_revs] = {}\n",
    "                    dataDict[pair_info_revs][\"pair_info\"] = pair_info_revs\n",
    "                    dataDict[pair_info_revs][\"pair_id\"] = dataDict[pair_info][\"pair_id\"]\n",
    "                    dataDict[pair_info_revs][\"constant_smiles\"] = dataDict[pair_info][\"constant_smiles\"]\n",
    "                    dataDict[pair_info_revs][\"pair_detail\"] = {key: [] for key in dataDict[pair_info][\"pair_detail\"]}\n",
    "\n",
    "                    dataDict[pair_info_revs][\"From_mol_id\"] = dataDict[pair_info][\"To_mol_id\"]\n",
    "                    dataDict[pair_info_revs][\"To_mol_id\"] = dataDict[pair_info][\"From_mol_id\"]\n",
    "                    dataDict[pair_info_revs][\"From_Structure\"] = dataDict[pair_info][\"To_Structure\"]\n",
    "                    dataDict[pair_info_revs][\"from_smiles\"] = dataDict[pair_info][\"to_smiles\"]\n",
    "                    dataDict[pair_info_revs][\"to_smiles\"] = dataDict[pair_info][\"from_smiles\"]\n",
    "                    \n",
    "                    for tmp_key in dataDict[pair_info]:\n",
    "                        if tmp_key[0:6] == 'Delta_':\n",
    "                            try:\n",
    "                                delta_symetric = dataDict[pair_info][tmp_key] * -1\n",
    "                            except Exception as e:\n",
    "                                delta_symetric = np.nan\n",
    "                            dataDict[pair_info_revs][tmp_key] = delta_symetric\n",
    "\n",
    "                        elif tmp_key[0:5] == 'From_':\n",
    "                            tmp_key_reverse = 'To_' + tmp_key[5:]\n",
    "                            dataDict[pair_info_revs][tmp_key_reverse] = dataDict[pair_info][tmp_key]\n",
    "                        \n",
    "                        elif tmp_key[0:3] == \"To_\":\n",
    "                            tmp_key_reverse = 'From_' + tmp_key[3:]\n",
    "                            dataDict[pair_info_revs][tmp_key_reverse] = dataDict[pair_info][tmp_key]\n",
    "                        else:\n",
    "                            pass\n",
    "            else:\n",
    "                ## this pair was removed from check list because it's the revs pair of another pair\n",
    "                pass\n",
    "        print(f\"\\t\\tNew num_pairs in symetric dataDict: {len(dataDict)}\")\n",
    "    \n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"==> Step 5 <Final data clean> complete, costs time = %ds ................\\n\" % (costTime))\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = Step_5_MMPs_DataClean(dataDict_tables, add_symetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
