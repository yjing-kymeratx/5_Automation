{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import shutil\n",
    "import chardet\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from d360api import d360api\n",
    "\n",
    "# dateToday = datetime.datetime.today().strftime('%Y%b%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 0. Prep args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------------------------------------------------\n",
    "my_query_id = 3539 # 2905\n",
    "\n",
    "colName_mid = 'Compound Name'\n",
    "colName_smi = 'Structure'    # 'Smiles'\n",
    "# colName_proj = 'Concat;Project'\n",
    "# colName_eid = 'Concat;External Id'\n",
    "\n",
    "## ------------------------------------------------------------------\n",
    "dict_prop_cols = {\n",
    "    'Permeability': 'ADME MDCK(WT) Permeability;Mean;A to B Papp (10^-6 cm/s);', \n",
    "    'Efflux': 'ADME MDCK (MDR1) efflux;Mean;Efflux Ratio;', \n",
    "    'Bioavailability': 'ADME PK;Mean;F %;Dose: 10.000 (mg/kg);Route of Administration: PO;Species: Rat;', \n",
    "    'Cl_obs': 'Copy 1 ;ADME PK;Mean;Cl_obs(mL/min/kg);Dose: 2.000 (mg/kg);Route of Administration: IV;Species: Rat;',\n",
    "    'hERG_IC50': 'ADME Tox-manual patch hERG 34C;GMean;m-patch hERG IC50 [uM];',\n",
    "    'hERG_eIC50': 'ADME Tox-manual patch hERG 34C;Concat;Comments',\n",
    "    'estFa': 'Not Availale',\n",
    "    'MW': 'Molecular Weight',\n",
    "    'bpKa1': 'in Silico PhysChem Property;Mean;Corr_ChemAxon_bpKa1;',\n",
    "    'logD': 'in Silico PhysChem Property;Mean;Kymera ClogD (v1);', \n",
    "    }\n",
    "## ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folderChecker(my_folder='./my_folder'):\n",
    "    # Check if the folder exists\n",
    "    check_folder = os.path.isdir(my_folder)\n",
    "    # os.path.exists(dir_outputs)\n",
    "    # If the folder does not exist, create it\n",
    "    if not check_folder:\n",
    "        os.makedirs(my_folder)\n",
    "        print(f\"\\tCreated folder:\", my_folder)\n",
    "    else:\n",
    "        print(f'{my_folder} is existing')\n",
    "    return my_folder\n",
    "\n",
    "## create tmp folder\n",
    "tmp_folder = folderChecker(f\"./tmp\")\n",
    "output_folder = folderChecker(f\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "############################ Step-1. download & load data from D360 ############################\n",
    "################################################################################################\n",
    "def _dataDownload(my_query_id=2905, user_name=\"yjing@kymeratx.com\", tokenFile='yjing_D360.token'):\n",
    "    # Create API connection to the PROD server\n",
    "    my_d360 = d360api(provider=\"https://10.3.20.47:8080\")  # PROD environment\n",
    "    user_name = user_name\n",
    "    tokenFile = tokenFile\n",
    "    \n",
    "    with open(tokenFile, 'r') as ofh:\n",
    "        service_token = ofh.readlines()[0]\n",
    "\n",
    "    # Authenticate connection using service token\n",
    "    my_d360.authenticate_servicetoken(servicetoken=service_token, user=user_name)\n",
    "    results = my_d360.download_query_results(query_id=my_query_id)\n",
    "    return results\n",
    "\n",
    "##--------------------------------------------------------------\n",
    "def determine_encoding(dataFile):\n",
    "    # Step 1: Open the CSV file in binary mode\n",
    "    with open(dataFile, 'rb') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Step 2: Detect the encoding using the chardet library\n",
    "    encoding_result = chardet.detect(data)\n",
    "\n",
    "    # Step 3: Retrieve the encoding information\n",
    "    encoding = encoding_result['encoding']\n",
    "\n",
    "    # Step 4: Print/export the detected encoding information\n",
    "    # print(\"Detected Encoding:\", encoding)\n",
    "    return encoding\n",
    "\n",
    "################################################################################################\n",
    "def Step_1_load_data(my_query_id, tmp_folder=\"./tmp\"):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    ## ------------------------------------------------------------------\n",
    "    ## download data from D360 using API\n",
    "    dataTableFileName = dataDownload(my_query_id=my_query_id)\n",
    "    print(f'\\tAll data have been downloaded in file {dataTableFileName}')\n",
    "\n",
    "    ## move the csv file to tmp folder\n",
    "    dataFile = f\"{tmp_folder}/{dataTableFileName}\"\n",
    "    shutil.move(dataTableFile_Raw, dataFile)\n",
    "    print(f\"\\tMove the downloaded file {dataTableFileName} to {dataFile}\")\n",
    "\n",
    "    ## determine encoding type\n",
    "    encoding = determine_encoding(dataFile)\n",
    "\n",
    "    ## read csv file\n",
    "    try:\n",
    "        dataTable = pd.read_csv(dataFile, encoding=encoding).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(f'\\tError: cannot read output file {dataFile}; error msg: {e}')\n",
    "        dataTable = None\n",
    "    else:\n",
    "        print(f'\\tThe downloaded data have data shape {dataTable.shape}')\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"====>The step 1 costs time = %ds ................\" % (costTime))\n",
    "    \n",
    "    return dataTable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "###################### Step-2. clean up data and calculate property ############################\n",
    "################################################################################################\n",
    "## ------------------------------------------------------------------\n",
    "def CheckThePropertyDataStats(dataTable, col_prop_prefix, propName):\n",
    "    col_mod, col_num = f\"{col_prop_prefix}(Mod)\", f\"{col_prop_prefix}(Num)\"\n",
    "    if (col_mod in dataTable) and (col_num in dataTable):\n",
    "        cond_1 = (dataTable[col_mod]=='=')\n",
    "        cond_2 = (dataTable[col_num].notna())\n",
    "        # print(dataTable[cond_1].shape, dataTable[cond_2].shape)\n",
    "        data_size_available = dataTable[cond_1 & cond_2].shape[0]\n",
    "        print(f\"\\tThere are total {data_size_available} existing data for {propName}\")\n",
    "        passCheck = True\n",
    "    else:\n",
    "        print(f\"\\tWarning! The column {col_prop_prefix}(Mod)/(Num) is not in the table.\")\n",
    "        passCheck = False\n",
    "    return passCheck\n",
    "\n",
    "## ------------------------------------------------------------------\n",
    "def clean_up_prop_data(row, col_prop_prefix, propName):\n",
    "    colName_mod = f\"{col_prop_prefix}(Mod)\"\n",
    "    colName_num = f\"{col_prop_prefix}(Num)\"\n",
    "\n",
    "    if row[colName_mod] == '=' and row.notna()[colName_num]:\n",
    "        result = row[colName_num] \n",
    "    else:\n",
    "        result = np.nan\n",
    "    return result\n",
    "\n",
    "## ----------------------------- F% and EstFa -------------------------------------\n",
    "def rm_elacridar_records(row, col_perctgF='Bioavailability', col_vehicle='ADME PK;Concat;Vehicle'):\n",
    "    result = row[col_perctgF]\n",
    "    if row.notna()[col_vehicle]:\n",
    "        if 'elacridar' in row[col_vehicle]:\n",
    "            result = np.nan\n",
    "            print(f\"\\t------>change from {row[col_perctgF]} to np.nan, {row[col_vehicle]}\")\n",
    "    return result\n",
    "\n",
    "def calc_EstFa_fromAdm(PKF_PO, Clobs_IV, Species='Rat'):\n",
    "    dict_IV_ratio = {'Rat': 90, 'Mouse': 70, 'Dog': 30, 'Monkey': 44}    \n",
    "    try:\n",
    "        estfa = (PKF_PO/100)/(1-(Clobs_IV/dict_IV_ratio[Species]))\n",
    "    except Exception as e:\n",
    "        estfa = np.nan\n",
    "    return estfa\n",
    "\n",
    "def calc_EstFa(row, colName_pctF, colName_Clobs, Species='Rat'):\n",
    "    try:\n",
    "        pctgF_PO, Clobs_IV = row[colName_pctgF], row[colName_Clobs]\n",
    "    except Exception as e:\n",
    "        # print(f\"\\tWarning! Cannot get data for this row from column <{colName_pctgF}> or <{colName_Clobs}>\")\n",
    "        result = np.nan\n",
    "    else:\n",
    "        result = calc_EstFa(pctgF_PO, Clobs_IV, Species=Species)\n",
    "    return result\n",
    "\n",
    "## ----------------------------- hERG -------------------------------------\n",
    "def calc_mean(value_list):\n",
    "    value_list_clean = []\n",
    "    for v in value_list:\n",
    "        if v not in [None, np.nan, '', ' ']:\n",
    "            try:\n",
    "                v_num = float(v)\n",
    "            except Exception as e:\n",
    "                print(f'\\tError, cannot numericalize value {v}', e)\n",
    "            else:\n",
    "                value_list_clean.append(v_num)\n",
    "    return np.mean(value_list_clean)\n",
    "\n",
    "def calc_eIC50_hERG_from_cmt(comments_str):\n",
    "    # e.g., comments_str = '21.38% inhibition @ 10 ?M' or '11.17 inhibition @ 3 ?M'\n",
    "    try:\n",
    "        [str_inhb, str_conc] = comments_str.split('@')\n",
    "\n",
    "        if '%' in str_inhb:\n",
    "            inhb = str_inhb.split('%')[0]\n",
    "        elif 'inhibit' in str_inhb:\n",
    "            inhb = str_inhb.split('inhibit')[0]\n",
    "        else:\n",
    "            inhb = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            inhb = float(inhb)\n",
    "        except:\n",
    "            eIC50 = None\n",
    "        else:\n",
    "            inhb = 0.1 if inhb < 0 else (99.99 if inhb > 100 else inhb)\n",
    "            conc = float(str_conc.split('M')[0][:-1])\n",
    "            eIC50 = conc*(100-inhb)/inhb\n",
    "            \n",
    "    except Exception as e:\n",
    "        eIC50 = None\n",
    "        if comments_str not in [' ', '/']:\n",
    "            print(f'\\tError, cannot calc hERG eIC50 from comment data. {comments_str}')\n",
    "    return eIC50\n",
    "\n",
    "def calc_hERG_eIC50(row, col_hERG_cmts):\n",
    "    if col_hERG_cmts in row:\n",
    "        if row.notna()[col_hERG_cmts]:\n",
    "            hERG_eIC50_list = []\n",
    "            for cmnt in row[col_hERG_cmts].split(';'):\n",
    "                this_eIC50 = calc_eIC50_hERG_from_cmt(cmnt)\n",
    "                hERG_eIC50_list.append(this_eIC50)\n",
    "            hERG_eIC50 = calc_mean(hERG_eIC50_list)\n",
    "        else:\n",
    "            result = np.nan\n",
    "            # print(f\"\\tNo data in this row for column <{col_hERG_cmts}>\")\n",
    "    else:\n",
    "        result = np.nan\n",
    "        print(f\"\\tColumn <{col_hERG_cmts}> is not in the Table\")\n",
    "\n",
    "def calc_hERG_mIC50(row, col_hERG_IC50, col_hERG_eIC50):\n",
    "    if row.notna()[col_hERG_IC50]:\n",
    "        result = row[col_hERG_IC50]\n",
    "    elif row.notna()[col_hERG_eIC50]:\n",
    "        result = row[col_hERG_eIC50]\n",
    "    else:\n",
    "        result = np.nan\n",
    "    return result\n",
    "\n",
    "################################################################################################\n",
    "def Step_2_clean_data(dataTable, dict_prop_cols, colName_mid, colName_smi, tmp_folder=\"./tmp\"):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    ## ------------------------------------------------------------------\n",
    "    dataTable = dataTable.dropna(subset=[colName_mid, colName_smi]).reset_index(drop=True)\n",
    "    print(f'There are total {dataTable.shape[0]} molecules with SMILES<{colName_smi}>')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    for prop in dict_prop_cols:\n",
    "        passCheck = CheckThePropertyDataStats(dataTable, col_prop_prefix=dict_prop_cols[prop], propName=prop)\n",
    "        if passCheck:\n",
    "            dataTable[prop] = dataTable.apply(lambda row: clean_up_prop_data(row, col_prop_prefix=dict_prop_cols[prop], propName=prop), axis=1)\n",
    "\n",
    "        ## remove the 'elacridar' records\n",
    "        if prop == 'Bioavailability':\n",
    "            print(f\"\\t==>The num rows with cleaned {prop} data (raw) is:\", str(dataTable[dataTable[prop].notna()].shape[0]))\n",
    "            dataTable[prop] = dataTable.apply(lambda row: rm_elacridar_records(row, col_perctgF=prop, col_vehicle='ADME PK;Concat;Vehicle'), axis=1)\n",
    "            print(f\"\\t==>The num rows with cleaned {prop} data (no elacridar) is:\", str(dataTable[dataTable[prop].notna()].shape[0]))\n",
    "\n",
    "        ## calc estFa\n",
    "        if prop == 'estFa':\n",
    "            dataTable[prop] = dataTable.apply(lambda row: calc_EstFa(row, 'Bioavailability', 'Cl_obs', Species='Rat'), axis=1)\n",
    "\n",
    "        ## calc hERG eIC50\n",
    "        if prop == 'hERG_eIC50':\n",
    "            dataTable[prop] = dataTable.apply(lambda row: calc_hERG_eIC50(row, dict_prop_cols[prop]), axis=1)\n",
    "            dataTable['hERG_mixedIC50'] = dataTable.apply(lambda row: calc_hERG_mIC50(row, 'hERG_IC50', 'hERG_eIC50'), axis=1)\n",
    "\n",
    "        ## rename MW\n",
    "        if prop == 'MW':\n",
    "            dataTable[prop] = dataTable[dict_prop_cols[prop]].apply(lambda x: x)\n",
    "\n",
    "        ## report\n",
    "        print(f\"\\t==>The num rows with cleaned {prop} data is:\", str(dataTable[dataTable[prop].notna()].shape[0]))\n",
    "    \n",
    "    ## ------------------------------------------------------------------\n",
    "    colNames_basic = [colName_mid, colName_smi]\n",
    "    colName_props = list(dict_prop_cols.keys())\n",
    "    dataTable_4_mmp = dataTable[colNames_basic + colName_props]\n",
    "\n",
    "    dateToday = datetime.datetime.today().strftime('%Y%b%d')\n",
    "    dataTable_4_mmp.to_csv(f'{tmp_folder}/Data_4_MMP_{dateToday}.csv', index=False)\n",
    "    print(f'\\tThe cleaned dataTable have data shape {dataTable_4_mmp.shape}')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"====>The step 2 costs time = %ds ................\" % (costTime))\n",
    "    return dataTable_4_mmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "################################### Step-3. MMPs analysis ######################################\n",
    "################################################################################################\n",
    "## ---------------- prepare the Smiles file and property file ----------------\n",
    "def prep_smi_file(dataTable, colName_prop_list, colName_mid='Compound Name', colName_smi='Structure', output_folder='./results'):\n",
    "    print(f\"\\tNow starting preparing the SMILES file and property CSV file for mmpdb ...\")\n",
    "    \n",
    "    ## the SMILES file for fragmentation\n",
    "    file_smi = f'{output_folder}/Compounds_All.smi'\n",
    "    file_prop_csv = f'{output_folder}/Property_All.csv'\n",
    "    delimiter_smi=' '\n",
    "    ##\n",
    "    data_dict_prop = {}\n",
    "    with open(file_smi, \"w\") as output_file:\n",
    "        # output_file.write(f'SMILES{delimiter_smi}ID' + \"\\n\")\n",
    "        for idx in dataTable.index:\n",
    "            mol_id = dataTable[colName_mid][idx]\n",
    "            mol_smi = dataTable[colName_smi][idx]\n",
    "\n",
    "            ## prepare the SMILES output\n",
    "            this_line = f'{mol_smi}{delimiter_smi}{mol_id}'\n",
    "            output_file.write(this_line + \"\\n\")  # Add a newline character after each string\n",
    "\n",
    "            ## prepare the property CSV output as dict\n",
    "            data_dict_prop[idx] = {}\n",
    "            data_dict_prop[idx]['ID'] = mol_id\n",
    "\n",
    "            for prop_name in colName_prop_list:\n",
    "                try:\n",
    "                    if dataTable_raw[prop_name].notna()[idx]:\n",
    "                        mol_prop = float(dataTable_raw[prop_name][idx])\n",
    "                    else:\n",
    "                        mol_prop = \"*\"\n",
    "                except Exception as e:\n",
    "                    data_dict_prop[idx][prop_name] = \"*\"\n",
    "                    # print(f'\\tThis mol {mol_id} does not have a proper property value: {e}')\n",
    "                else:\n",
    "                    data_dict_prop[idx][prop_name] = mol_prop\n",
    "        \n",
    "    print(f'\\tThe SMILES strings have been saved into .smi file: {file_smi}')\n",
    "        \n",
    "    ## save the csv results\n",
    "    data_table_prop = pd.DataFrame.from_dict(data_dict_prop).T\n",
    "    data_table_prop.to_csv(file_prop_csv, index=False, sep=delimiter)\n",
    "    print(f'\\tThe property data ({data_table_prop.shape}) have been saved into .csv file: {file_smi}')\n",
    "    # data_table_prop.head(3)\n",
    "    return file_smi, file_prop_csv\n",
    "        \n",
    "## ---------------- basic cmd run ----------------\n",
    "def run_cmd(commandLine):\n",
    "    # beginTime = time.time()\n",
    "\n",
    "    # Use subprocess to execute the command\n",
    "    process = subprocess.Popen(commandLine, stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    # costTime = time.time()-beginTime\n",
    "    # print(f\"\\tThis command costs time = %ds ................\" % (costTime))\n",
    "    return (output, error)\n",
    "\n",
    "################################################################################################\n",
    "def Step_3_mmp_analysis(dataTable, dict_prop_cols, colName_mid='Compound Name', colName_smi='Structure', output_folder='./results'):\n",
    "    ## count time\n",
    "    beginTime = time.time()\n",
    "    ## ------------------------------------------------------------------    \n",
    "    ## prepare the Smiles file and property file\n",
    "    colName_prop_list = list(dict_prop_cols)\n",
    "    file_smi, file_prop_csv = prep_smi_file(dataTable, colName_prop_list, colName_mid, colName_smi, output_folder)\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    ## Fragment the SMILES\n",
    "    file_fragdb = f'{output_folder}/Compounds_All.fragdb'\n",
    "    commandLine_1 = ['mmpdb', 'fragment', file_smi, '-o', file_fragdb]\n",
    "    (output_1, error_1) = run_cmd(commandLine_1)\n",
    "    print(f'\\tThe fragmentation is completed and saved into file {file_fragdb}')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    ## Indexing to find the MMPs in the fragment file & Load the activity/property data\n",
    "    file_mmpdb = f'{output_folder}/Compounds_All.mmpdb'\n",
    "    commandLine_2 = ['mmpdb', 'index', file_fragdb, '-o', file_mmpdb, '--properties', file_prop_csv]\n",
    "    (output_2, error_2) = run_cmd(commandLine_2)\n",
    "    print(f'\\tThe indexing/mmp generation is completed and saved into file {file_mmpdb}')\n",
    "\n",
    "    ## ------------------------------------------------------------------\n",
    "    costTime = time.time()-beginTime\n",
    "    print(f\"====>The step 3 costs time = %ds ................\" % (costTime))\n",
    "\n",
    "    return file_mmpdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "########################################## main ################################################\n",
    "################################################################################################\n",
    "\n",
    "#### Step-1. download & load data from D360\n",
    "print(f\"Step 1: download & load data from D360 ...\")\n",
    "dataTable = Step_1_load_data(my_query_id, tmp_folder)\n",
    "\n",
    "## step 1\n",
    "dataTable.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step-2. clean up data and calculate property\n",
    "print(f\"Step 2: clean up data and calculate new property ...\")\n",
    "dataTable_4_mmp = Step_2_clean_data(dataTable, dict_prop_cols, colName_mid, colName_smi, tmp_folder)\n",
    "\n",
    "## step 2\n",
    "dataTable_4_mmp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step-3. MMPs analysis\n",
    "print(f\"Step 3: run MMP analysis using mmpdb ...\")\n",
    "file_mmpdb = Step_3_mmp_analysis(dataTable_4_mmp, dict_prop_cols, colName_mid, colName_smi, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mmpdb list ./results/Compounds_All.mmpdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mmpdb generate --smiles 'CCN(C)C(=O)C1=CC2=C(N1)C(F)=CN=C2C1=C(Cl)C=C(N2CCC(CN3CCN(C4=CC=CC5=C4N(C)C(=O)N5C4CCC(=O)NC4=O)CC3)CC2)C=C1' ./results/Compounds_All.mmpdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
