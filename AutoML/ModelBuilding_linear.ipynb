{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warining\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from sklearn.exceptions import ConvergenceWarning\n",
    "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining_X: (24, 23); Training_y: (24,)\n",
      "\tValidation_X: (3, 23); Validation_y: (3,)\n",
      "\tTest_X: (3, 23); Test_y: (3,)\n"
     ]
    }
   ],
   "source": [
    "fileNameIn = './results/data_input_4_ModelBuilding.csv'\n",
    "colName_mid = 'Compound Name'\n",
    "colName_split = 'Split'\n",
    "colName_y = 'ADME MDCK(WT) Permeability;Mean;A to B Papp (10^-6 cm/s);(Num)'\n",
    "sep = ','\n",
    "\n",
    "import os\n",
    "folderPathOut = './results'    ## './results'\n",
    "os.makedirs(folderPathOut, exist_ok=True)    \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "dataTable_raw = pd.read_csv(fileNameIn, sep=sep)\n",
    "colName_X = [col for col in dataTable_raw.columns if col not in [colName_mid, colName_split, colName_y]]\n",
    "\n",
    "## training\n",
    "dataTable_train = dataTable_raw[dataTable_raw[colName_split]=='Training']\n",
    "X_train, y_train = dataTable_train[colName_X], dataTable_train[colName_y]\n",
    "print(f\"\\tTraining_X: {X_train.shape}; Training_y: {y_train.shape}\")\n",
    "\n",
    "## validation\n",
    "dataTable_val = dataTable_raw[dataTable_raw[colName_split]=='Validation']\n",
    "X_val, y_val = dataTable_val[colName_X], dataTable_val[colName_y]\n",
    "print(f\"\\tValidation_X: {X_val.shape}; Validation_y: {y_val.shape}\")\n",
    "\n",
    "## test\n",
    "dataTable_test = dataTable_raw[dataTable_raw[colName_split]=='Test']\n",
    "X_test, y_test = dataTable_test[colName_X], dataTable_test[colName_y]\n",
    "print(f\"\\tTest_X: {X_test.shape}; Test_y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <===================== model initiate =====================>\n",
    "def step_1_model_init(ml_methed, n_jobs=-1, rng=666666):\n",
    "    ml_methed = ml_methed.lower()\n",
    "    ## -------------------- random forest --------------------\n",
    "    if ml_methed in ['rf', 'random forest', 'randomforest']:\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        sk_model = RandomForestRegressor(random_state=rng, oob_score=True, n_jobs=n_jobs)\n",
    "        search_space = {'n_estimators': [50, 200, 500], 'max_depth': [2, 4, 6], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [5, 10, 25, 50], 'min_samples_split': [2, 5, 8, 10]}\n",
    "\n",
    "    ## -------------------- SVM --------------------\n",
    "    elif ml_methed in ['svm', 'support vector machine', 'supportvectormachine']:\n",
    "        from sklearn.svm import SVR\n",
    "        sk_model = SVR(kernel=\"rbf\", gamma=0.1)\n",
    "        search_space = {'kernel': ['poly', 'rbf', 'sigmoid'], 'gamma': ['scale', 'auto'], 'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "    ## -------------------- MLP --------------------\n",
    "    elif ml_methed in ['mlp', 'ann']:\n",
    "        from sklearn.neural_network import MLPRegressor\n",
    "        sk_model = MLPRegressor(random_state=rng, max_iter=100, early_stopping=True)\n",
    "        search_space = {'hidden_layer_sizes': [(128,), (128, 128), (128, 128, 128)], 'activation': ['logistic', 'tanh', 'relu'], 'solver': ['sgd', 'adam'], 'alpha': [0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "    ## -------------------- KNN --------------------\n",
    "    elif ml_methed in ['knn', 'k-nn', 'nearest neighbor', 'nearestneighbor']:\n",
    "        from sklearn.neighbors import KNeighborsRegressor\n",
    "        sk_model = KNeighborsRegressor(n_neighbors=3, n_jobs=n_jobs)\n",
    "        search_space = {'n_neighbors': [1, 3, 5, 10]}\n",
    "\n",
    "    ## -------------------- Linear --------------------\n",
    "    else:\n",
    "        if ml_methed != 'linear':\n",
    "            print(f\"Error! no proper ML methods were selected, using Linear method instead\")\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        sk_model = LinearRegression(n_jobs=n_jobs)\n",
    "        search_space = None\n",
    "\n",
    "    return sk_model, search_space\n",
    "\n",
    "## <===================== model training =====================>\n",
    "def _HyperParamSearch(sk_model, X, y, search_space=None, search_method='grid', scoring='neg_mean_absolute_error', nFolds=5, n_jobs=-1):\n",
    "    print(f\"\\t\\tStart Hyper-Parameter Tunning ...\")\n",
    "    SearchResults = {'best_model': None, 'best_score':None, 'best_param':None}\n",
    "\n",
    "    # if search_method == 'grid':\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    optimizer = GridSearchCV(estimator=sk_model, param_grid=search_space, scoring=scoring, cv=nFolds, n_jobs=n_jobs)\n",
    "    optimizer.fit(X, y)\n",
    "    ## search results\n",
    "    SearchResults['optimizer'] = optimizer    ## optimizer.best_estimator_, optimizer.best_score_\n",
    "    SearchResults['best_param'] = optimizer.best_estimator_.get_params()\n",
    "\n",
    "    ## export\n",
    "    print(f\"\\t\\tThe best {scoring}: {SearchResults['best_score']}\\n\\t\\tThe best hp-params: {SearchResults['best_param']}\")\n",
    "    print(f\"\\t\\tComplete Hyper-Parameter Tunning ...\")\n",
    "    return SearchResults\n",
    "\n",
    "##\n",
    "def step_2_model_training(sk_model, X, y, logy=False, doHPT=False, search_space=None, scoring='neg_mean_absolute_error', n_jobs=-1):\n",
    "    import time   \n",
    "    beginTime = time.time()        \n",
    "    ## ----------------------------------------------------------------\n",
    "    import numpy as np\n",
    "    # X = X.to_numpy()\n",
    "    y = y.to_numpy().reshape((len(y), ))\n",
    "    y = np.log10(y) if logy else y\n",
    "\n",
    "    ## ----- hyper parameter search ----------------\n",
    "    if doHPT and search_space is not None:\n",
    "        HPSearchResults = _HyperParamSearch(sk_model, X, y, search_space, search_method='grid', scoring=scoring, nFolds=5, n_jobs=n_jobs)\n",
    "        sk_model = sk_model.set_params(**HPSearchResults['best_param'])    #optimizer.best_estimator_\n",
    "\n",
    "    ## ----- fit the model -----\n",
    "    sk_model.fit(X, y)\n",
    "\n",
    "    ## ----------------------------------------------------------------        \n",
    "    print(f\"\\tThe model training costs time = {(time.time()-beginTime):.2f} s ................\")\n",
    "    return sk_model\n",
    "\n",
    "## <===================== model predict =====================>\n",
    "def step_3_make_prediction(sk_model, X, logy=False):\n",
    "    y_pred = sk_model.predict(X)\n",
    "    y_pred = 10**y_pred if logy else y_pred\n",
    "    return y_pred\n",
    "\n",
    "## <===================== model predict =====================>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tThe model training costs time = 0.03 s ................\n",
      "\t\tStart Hyper-Parameter Tunning ...\n",
      "\t\tThe best neg_mean_absolute_error: None\n",
      "\t\tThe best hp-params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 2, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 5, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 500, 'n_jobs': -1, 'oob_score': True, 'random_state': 666666, 'verbose': 0, 'warm_start': False}\n",
      "\t\tComplete Hyper-Parameter Tunning ...\n",
      "\tThe model training costs time = 28.76 s ................\n",
      "\t\tStart Hyper-Parameter Tunning ...\n",
      "\t\tThe best neg_mean_absolute_error: None\n",
      "\t\tThe best hp-params: {'C': 10, 'cache_size': 200, 'coef0': 0.0, 'degree': 3, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "\t\tComplete Hyper-Parameter Tunning ...\n",
      "\tThe model training costs time = 0.20 s ................\n",
      "\t\tStart Hyper-Parameter Tunning ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/mnt/data0/software/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tThe best neg_mean_absolute_error: None\n",
      "\t\tThe best hp-params: {'activation': 'tanh', 'alpha': 0.1, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (128, 128), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 100, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 666666, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\t\tComplete Hyper-Parameter Tunning ...\n",
      "\tThe model training costs time = 0.89 s ................\n",
      "\t\tStart Hyper-Parameter Tunning ...\n",
      "\t\tThe best neg_mean_absolute_error: None\n",
      "\t\tThe best hp-params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}\n",
      "\t\tComplete Hyper-Parameter Tunning ...\n",
      "\tThe model training costs time = 0.03 s ................\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "n_jobs = -1\n",
    "rng = 666666\n",
    "logy = True\n",
    "doHPT = True\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "ml_methed = 'rf'\n",
    "ml_methed_list = ['linear', 'rf', 'svm', 'mlp', 'knn']\n",
    "\n",
    "model_dict = {'data': {'Training': [X_train, y_train], 'Validation': [X_val, y_val],  'Test': [X_test, y_test]},\n",
    "              'config': {'rng': rng, 'n_jobs':n_jobs, 'logy': logy, 'doHPT': doHPT}, \n",
    "              'model': {},\n",
    "              'results': None}\n",
    "\n",
    "##\n",
    "for ml_methed in ml_methed_list:\n",
    "    ## training\n",
    "    sk_model, search_space = step_1_model_init(ml_methed, n_jobs=n_jobs, rng=rng)\n",
    "    sk_model = step_2_model_training(sk_model, X_train, y_train, logy=logy, doHPT=doHPT, search_space=search_space, scoring=scoring, n_jobs=n_jobs) \n",
    "    model_dict['model'][ml_methed] = sk_model\n",
    "\n",
    "    ## prediction\n",
    "    col_pred = f\"Prediction_{ml_methed}_{colName_y}\"\n",
    "    dataTable_train[col_pred] = step_3_make_prediction(sk_model, X_train, logy=logy)\n",
    "    dataTable_val[col_pred] = step_3_make_prediction(sk_model, X_val, logy=logy)\n",
    "    dataTable_test[col_pred] = step_3_make_prediction(sk_model, X_test, logy=logy)\n",
    "\n",
    "## merge/concact data\n",
    "dataTable_list = [dataTable_train, dataTable_val, dataTable_test]\n",
    "dataTable_results = pd.concat(dataTable_list).sort_index(ascending=True)\n",
    "model_dict['results'] = dataTable_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save results\n",
    "fileNameOut_pred = f\"{folderPathOut}/prediction_results.csv\"\n",
    "dataTable_results.to_csv(fileNameOut_pred, index=False)\n",
    "\n",
    "## save model\n",
    "import pickle\n",
    "fileNameOut_model = f\"{folderPathOut}/ml_models.pickle\"\n",
    "with open(fileNameOut_model, 'rb') as ofh_models:\n",
    "    pickle.dump(model_dict, ofh_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_train[f\"Predict_{colName_y}\"] = step_3_make_prediction(sk_model, X_train, logy=logy)\n",
    "dataTable_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_val[f\"Predict_{colName_y}\"] = step_3_make_prediction(sk_model, X_val, logy=logy)\n",
    "dataTable_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_test[f\"Predict_{colName_y}\"] = step_3_make_prediction(sk_model, X_test, logy=logy)\n",
    "dataTable_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _CalcScores(self, y_pred, y_true, printLog=True):   \n",
    "        dataDict_result = {}\n",
    "        try:\n",
    "            y_pred = y_pred.reshape((len(y_pred), ))\n",
    "            y_true = y_true.reshape((len(y_true), ))\n",
    "        except Exception as e:\n",
    "            print(f\"\\tError! Cannot reformatting the y_pred and y_true when calculating the statistics\")\n",
    "        else:\n",
    "            ## calculate the mean absolute error using Scikit learn\n",
    "            try:\n",
    "                dataDict_result['MAE'] = mean_absolute_error(y_true, y_pred)\n",
    "            except:\n",
    "                dataDict_result['MAE'] = np.nan\n",
    "            \n",
    "            ## calculate the PearsonCorrelationCoefficient\n",
    "            try:\n",
    "                pr_np = np.corrcoef(y_pred, y_true)[1, 0]\n",
    "                dataDict_result['Pearson_R2'] = pr_np * pr_np\n",
    "            except:\n",
    "                dataDict_result['Pearson_R2'] = np.nan\n",
    "\n",
    "            ## calculate the rank-order correlation (Spearman's rho)\n",
    "            try:\n",
    "                sr_sp, sp_sp = spearmanr(y_pred, y_true)[0], spearmanr(y_pred, y_true)[1]\n",
    "                dataDict_result['Spearman_R2'] = sr_sp * sr_sp\n",
    "            except:\n",
    "                dataDict_result['Spearman_R2'], sp_sp = np.nan, np.nan\n",
    "                        \n",
    "            ## calculate the # Kendall's tau\n",
    "            try:\n",
    "                kr_sp, kp_sp = kendalltau(y_pred, y_true)[0] , kendalltau(y_pred, y_true)[1]\n",
    "                dataDict_result['KendallTau_R2'] = kr_sp * kr_sp\n",
    "            except:\n",
    "                dataDict_result['KendallTau_R2'], kp_sp = np.nan, np.nan\n",
    "             \n",
    "            ## print out the results\n",
    "            if printLog:\n",
    "                print(f\"\\t\\tData shape: y_pred {y_pred.shape}; y_true {y_true.shape}\")\n",
    "                print(f\"\\t\\tMean absolute error: {dataDict_result['MAE']:.2f}\")\n",
    "                print(f\"\\t\\tPearson-R2: {dataDict_result['Pearson_R2']:.2f}\")\n",
    "                print(f\"\\t\\tSpearman-R2: {dataDict_result['Spearman_R2']:.2f} (p={sp_sp:.2f})\")\n",
    "                print(f\"\\t\\tKendall-R2: {dataDict_result['KendallTau_R2']:.2f} (p={kp_sp:.2f})\")\n",
    "        return dataDict_result\n",
    "\n",
    "\n",
    "\n",
    "def step_3_model_evaluating(sk_model, X, y, logy=False):\n",
    "    y_pred = sk_model.predict(X)\n",
    "    y_pred = float(10**y_pred) if logy else y_pred\n",
    "\n",
    "\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(sk_model, X, logy=False):\n",
    "    y_pred = sk_model.predict(X)\n",
    "    y_pred = float(10**y_pred) if logy else y_pred\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scoring = pd.read_csv('./results/feature_scoring_merged.csv')\n",
    "feature_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataTable_pred = pd.read_csv('./Data/DataView_MDCK_MDR1__Permeability_1__export_top30.csv')\n",
    "print(dataTable_pred.shape)\n",
    "\n",
    "from DescGen import desc_calculator_chemaxon, calc_desc_from_table\n",
    "cx_version, cx_desc = 'V22', 'all'\n",
    "calculator_cx = desc_calculator_chemaxon(version=cx_version, desc_list=cx_desc)\n",
    "result_dict_cx= calc_desc_from_table(dataTable_pred, colName_mid='Compound Name', colName_smi='Smiles', desc_calculator=calculator_cx)\n",
    "dataTable_pred_prop = pd.DataFrame.from_dict(result_dict_cx).T\n",
    "dataTable_pred_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_Model(object):\n",
    "    ## <===================== model initiation =====================>\n",
    "    def __init__(self,  myScikitModel=None, modelName='Regression_Model', log_y=True, rng=666666, n_jobs=-1):\n",
    "        assert myScikitModel is not None, f\"\\tWarning! Please define an initiated RDKit ML model\"\n",
    "        self._name = modelName\n",
    "        self._rng = rng\n",
    "        self._n_jobs = n_jobs\n",
    "        self.model = myScikitModel\n",
    "        self.log_y = log_y\n",
    "        self.HPT_Results = {}\n",
    "        self.predictions = None\n",
    "        self.performance = {}\n",
    "        self.plots = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the class of custom ML regression/classification models based on Scikit-learn. \n",
    "\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##\n",
    "from scipy.stats import linregress, t, pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# from skopt import BayesSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, auc, roc_curve, accuracy_score, RocCurveDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## custom modules\n",
    "\n",
    "\n",
    "    def PostProcess_y(self, dataTable_y):\n",
    "        assert len(dataTable_y.columns) == 1, f\"Error! The <dataTable_y> has incorrect {len(y.columns)} columns: {y.columns}\"\n",
    "        colName_raw = dataTable_y.columns[0]\n",
    "        y = copy.deepcopy(dataTable_y)\n",
    "\n",
    "        if self.transformType=='log10':\n",
    "            y['y_postprocess'] = y[colName_raw].apply(lambda x: 10**(x))\n",
    "        \n",
    "        elif self.transformType=='One-hot':\n",
    "            y['y_postprocess'] = y[colName_raw].apply(lambda x: dataDict_b2v(str(x)))\n",
    "        else:\n",
    "            y['y_postprocess'] = y[colName_raw].apply(lambda x: x)\n",
    "        return y\n",
    "    ## ============================================================\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "############################### Classification model ####################################\n",
    "#########################################################################################\n",
    "class Classification_Model(object):\n",
    "    ## <===================== model initiation =====================>\n",
    "    def __init__(self, myScikitModel=None, modelName='Classification_Model', rng=666666, n_jobs=-1):\n",
    "        assert myScikitModel is not None, f\"\\tWarning! Please define an initiated RDKit ML model\"\n",
    "        self._name = modelName\n",
    "        self._rng = rng\n",
    "        self._n_jobs = n_jobs\n",
    "        self.model = myScikitModel\n",
    "        self.HPT_Results = {}\n",
    "        self.predictions = None\n",
    "        self.performance = {}\n",
    "        self.plots = {}\n",
    "        self.best_threshold = 0.5\n",
    "        self.class_label = {0: \"0\", 1: \"0\"}\n",
    "\n",
    "    ## <===================== model training =====================>\n",
    "    def Train(self, X, y, printLog=True, HPT=False, search_space=None):\n",
    "        ## count time\n",
    "        beginTime = time.time()\n",
    "        ## ----------------------------------------------------------------\n",
    "        ## ------------ hyper parameter search ------------\n",
    "        if HPT:\n",
    "            self._HyperParamSearch(X, y, search_space=search_space, printLog=printLog)\n",
    "        \n",
    "        ## ------------ fit the model ------------\n",
    "        self.model.fit(X, y)\n",
    "       \n",
    "        ## ----------------------------------------------------------------\n",
    "        print(f\"\\tModel construction costs time = {(time.time()-beginTime):.2f} s ................\")\n",
    "        return None\n",
    "\n",
    "    ## <===================== model evaluation =====================>\n",
    "    def MakePrediction(self, X):\n",
    "        y_pred_prob = self.model.predict_proba(X)[:, 1]\n",
    "        y_pred = np.where(y_pred_prob >= self.best_threshold, 1, 0)\n",
    "        return y_pred, y_pred_prob\n",
    "    \n",
    "    def Evaluate(self, X, y, ds_label='TBD', estCutoff=False, printLog=True, plotResult=False):\n",
    "        ## make prediction\n",
    "        # y_pred = self.model.predict(X)    #####################\n",
    "        _, y_pred_prob = self.model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        ## calcualte statistics\n",
    "        print(f\"\\tEvaluation results of the {ds_label} dataset:\")\n",
    "        self.performance[ds_label] = self._CalcScores(y_pred_prob=y_pred_prob, y_true=y.to_numpy(), estTrsd=estCutoff, printLog=printLog)\n",
    "\n",
    "        ## save prediction\n",
    "        df_predictions = copy.deepcopy(y)\n",
    "        df_predictions['Experiment'] = df_predictions[y.columns[0]]\n",
    "        df_predictions['DataSet'] = ds_label\n",
    "        df_predictions['Prob_1'] = y_pred_prob\n",
    "        df_predictions['Prediction'] = df_predictions['Prob_1'].apply(lambda x: self._Pred_Class(x))\n",
    "        self.predictions = pd.concat([self.predictions, df_predictions]) if self.predictions is not None else df_predictions\n",
    "        \n",
    "        ## plotting\n",
    "        if plotResult:\n",
    "            self.plots[ds_label] = self._Plot_ROCAUC(ds_label)\n",
    "        return None\n",
    "\n",
    "    ## <===================== HPTunning =====================>\n",
    "    def _HyperParamSearch(self, X, y, search_space=None, search_method='grid', scoring='roc_auc', nFolds=5, printLog=True):\n",
    "        ## count time\n",
    "        beginTime = time.time()\n",
    "        ## --------------------------------\n",
    "        print(f\"\\tStart Hyper-Parameter Tunning ...\")\n",
    "        SearchResults = {'best_model': None, 'best_score':None, 'best_param':None}\n",
    "        \n",
    "        ##\n",
    "        if search_method == 'grid':\n",
    "            optimizer = GridSearchCV(estimator=self.model, param_grid=search_space, scoring=scoring, cv=nFolds, n_jobs=self._n_jobs)\n",
    "        elif search_method =='Bayes':\n",
    "            optimizer = GridSearchCV(estimator=self.model, param_grid=search_space, scoring=scoring, cv=nFolds, n_jobs=self._n_jobs)\n",
    "        else:\n",
    "            optimizer = GridSearchCV(estimator=self.model, param_grid=search_space, scoring=scoring, cv=nFolds, n_jobs=self._n_jobs)\n",
    "\n",
    "        ## fit the Optimizer to the Data\n",
    "        y_reshaped = y.to_numpy().reshape((len(y), ))\n",
    "        optimizer.fit(X, y_reshaped)\n",
    "\n",
    "        ## search results\n",
    "        SearchResults['best_model'] = optimizer.best_estimator_\n",
    "        SearchResults['best_score'] = optimizer.best_score_\n",
    "        SearchResults['best_param'] = SearchResults['best_model'].get_params()\n",
    "        self.HPT_Results[search_method] = SearchResults\n",
    "        \n",
    "        ##\n",
    "        # self.model = optimizer.best_estimator_\n",
    "        if SearchResults['best_param'] is not None:\n",
    "            self.model.set_params(**SearchResults['best_param'])\n",
    "        else:\n",
    "            self.model = self.model\n",
    "\n",
    "        if printLog:\n",
    "            print(f\"\\tThis is the log info\")\n",
    "            print(f\"\\tThe best {scoring}: {SearchResults['best_score']}\")\n",
    "            print(f\"\\tThe optimized Params: {SearchResults['best_param']}\")\n",
    "            ## ----------------------------------------------------------------\n",
    "            print(f\"\\tHyper-parameters Tunning costs time = {(time.time()-beginTime):.2f} s ................\")\n",
    "        return None\n",
    "    \n",
    "    ## <===================== tools =====================>    \n",
    "    def __CalcScore_ROCAUC(self, y_prob, y_true, estTrsd=False):\n",
    "        try:\n",
    "            ## Assuming y_true are the true labels and y_prob are the predicted probabilities\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "            aucs_score = auc(fpr, tpr)\n",
    "\n",
    "            ## determine the best threshold\n",
    "            if estTrsd:\n",
    "                ## Calculate the distance to the top-left corner (0,1)\n",
    "                distances = np.sqrt(fpr**2 + (1-tpr)**2)\n",
    "                self.best_threshold = thresholds[distances.argmin()]\n",
    "                print(f\"\\tThe best threshold is changged to {self.best_threshold}\")\n",
    "                # ## Calculate Youden's J statistic\n",
    "                # youden_j = tpr - fpr\n",
    "                # self.best_threshold = thresholds[youden_j.argmax()]\n",
    "        except Exception as e:\n",
    "            print(f'Warning! Cannot calculate ROC AUC, error msg: {e}')\n",
    "            auc_score, fpr, tpr, thresholds = np.nan, np.nan, np.nan, np.nan\n",
    "        results = {'auc_score': aucs_score, 'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds}\n",
    "        return results\n",
    "\n",
    "    def __CalcScore_ACC(self, y_pred, y_true):\n",
    "        try:\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "        except Exception as e:\n",
    "            acc = np.nan\n",
    "        return acc\n",
    "    \n",
    "    def __CalcScore_CM(self, y_pred, y_true):\n",
    "        try:\n",
    "            cm = confusion_matrix(Clas_test, Clas_test_pred)\n",
    "        except Exception as e:\n",
    "            cm = np.nan\n",
    "        return cm\n",
    "                \n",
    "    def _CalcScores(self, y_pred_prob, y_true, estTrsd=False, printLog=True):   \n",
    "        dataDict_result = {}\n",
    "        try:\n",
    "            y_pred = y_pred.reshape((len(y_pred), ))\n",
    "            y_true = y_true.reshape((len(y_true), ))\n",
    "        except Exception as e:\n",
    "            print(f\"\\tError! Cannot reformatting the y_pred and y_true when calculating the statistics\")\n",
    "        else:\n",
    "            ## calculate the ROC auc\n",
    "            dataDict_result['ROC_AUC'] = self.__CalcScore_ROCAUC(y_pred, y_true, estTrsd=estTrsd)\n",
    "            \n",
    "            ## calculate the accuracy\n",
    "            y_pred_binary = np.where(y_pred_prob >= self.best_threshold, 1, 0)\n",
    "            dataDict_result['Accuracy'] = self.__CalcScore_ACC(y_pred=y_pred_binary, y_true=y_true)\n",
    "            dataDict_result['ConfusionMatrics'] = self.__CalcScore_CM(y_pred=y_pred_binary, y_true=y_true)\n",
    "             \n",
    "            ## print out the results\n",
    "            if printLog:\n",
    "                print(f\"\\t\\tData shape: y_pred {y_pred.shape}; y_true {y_true.shape}\")\n",
    "                print(f\"\\t\\tAUROC: {dataDict_result['ROC_AUC']['auc_score']:.2f}\")\n",
    "                print(f\"\\t\\tAccuracy: {dataDict_result['Accuracy']:.2f}\")\n",
    "                print(f\"\\t\\tConfusionMatrics: {dataDict_result['ConfusionMatrics']}\")\n",
    "        return dataDict_result\n",
    "\n",
    "    def _Plot_ROCAUC(self, ds_label):\n",
    "        ## initiate the figure axes\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ## generate plot\n",
    "        try:\n",
    "            dataDict_roc = self.performance[ds_label]['ROC_AUC']\n",
    "            fpr, tpr, roc_auc = dataDict_roc['fpr'], dataDict_roc['tpr'], dataDict_roc['auc_score']\n",
    "            display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=ds_label)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        else:\n",
    "            display.plot(ax=ax)\n",
    "            ## set the figure config\n",
    "            ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], \n",
    "                xlabel=\"False Positive Rate\", ylabel=\"True Positive Rate\", \n",
    "                title=f\"ROC Curve ({ds_label})\")\n",
    "            ax.axis(\"square\")\n",
    "            ax.legend(loc=\"lower right\")\n",
    "            # plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def _Pred_Class(self, prob):\n",
    "        if prob >= self.best_threshold:\n",
    "            pred = 1 \n",
    "        else:\n",
    "            pred = 0\n",
    "        return pred\n",
    "\n",
    "    def ___futureFunctionsTBA():\n",
    "        return None\n",
    "\n",
    "#########################################################################################\n",
    "################################# select_ML_methods #####################################\n",
    "#########################################################################################\n",
    "\n",
    "def select_ML_methods(modelType, ml_methed, rng=666666, knnk=3, n_jobs=-1):\n",
    "    if modelType == 'regression':\n",
    "        if ml_methed == 'RF':\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            sk_model = RandomForestRegressor(random_state=rng, oob_score=True, n_jobs=n_jobs)\n",
    "            search_space = {\n",
    "                'n_estimators': [50, 100, 250, 500],\n",
    "                'max_depth': [2, 4, 6],\n",
    "                'max_features': ['auto', 'sqrt'],\n",
    "                'min_samples_leaf': [1, 5, 10, 25, 50],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'min_samples_split': [2, 5, 8, 10]}\n",
    "        \n",
    "        elif ml_methed == 'linear':\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            sk_model = LinearRegression(n_jobs=n_jobs)\n",
    "            search_space = None\n",
    "            # from sklearn.linear_model import Lasso\n",
    "            # sk_model = Lasso(alpha=0.1)\n",
    "            # search_space = {'alpha': [0, 0.1, 0.25, 0.5, 0.8]}\n",
    "        \n",
    "        elif ml_methed == 'SVM':\n",
    "            from sklearn.svm import SVR\n",
    "            sk_model = SVR(kernel=\"rbf\", gamma=0.1)\n",
    "            search_space = {\n",
    "                'kernel': ['poly', 'rbf', 'sigmoid'], \n",
    "                'gamma': ['scale', 'auto'], \n",
    "                'C': [0.1, 1, 10, 100]}\n",
    "        \n",
    "        elif ml_methed == 'MLP':\n",
    "            from sklearn.neural_network import MLPRegressor\n",
    "            sk_model = MLPRegressor(random_state=rng, max_iter=500, early_stopping=True)\n",
    "            search_space = {\n",
    "                'hidden_layer_sizes': [(128,), (128, 128), (128, 128, 128)], \n",
    "                'activation': ['logistic', 'tanh', 'relu'], \n",
    "                'solver': ['sgd', 'adam'],\n",
    "                'alpha': [0.1, 0.01, 0.001, 0.0001]}\n",
    "        \n",
    "        elif ml_methed == 'KNN':\n",
    "            from sklearn.neighbors import KNeighborsRegressor\n",
    "            sk_model = KNeighborsRegressor(n_neighbors=knnk, n_jobs=n_jobs)\n",
    "            search_space = {'n_neighbors': [1, 3, 5, 10]}\n",
    "        \n",
    "        else:\n",
    "            print(f\"Error! no proper ML methods were selected, using Linear method instead\")\n",
    "            from sklearn.linear_model import Lasso\n",
    "            sk_model = Lasso(alpha=0.1)\n",
    "            search_space = {'alpha': [0, 0.1, 0.25, 0.5, 0.8]}\n",
    "\n",
    "    elif modelType == 'classification':\n",
    "        if ml_methed == 'RF':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            sk_model = RandomForestClassifier(random_state=rng, class_weight='balanced_subsample', oob_score=True, n_jobs=n_jobs)\n",
    "            search_space = {\n",
    "                'n_estimators': [50, 100, 250, 500],\n",
    "                'max_depth': [2, 4, 6],\n",
    "                'max_features': ['auto', 'sqrt'],\n",
    "                'min_samples_leaf': [1, 5, 10, 25, 50],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'min_samples_split': [2, 5, 8, 10]}\n",
    "\n",
    "        elif ml_methed == 'linear':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            sk_model = LogisticRegression(random_state=rng, n_jobs=n_jobs)\n",
    "            search_space = None\n",
    "\n",
    "        elif ml_methed == 'SVM':\n",
    "            from sklearn.svm import SVC\n",
    "            sk_model = SVC(kernel=\"rbf\", gamma=0.1, random_state=rng, probability=True)\n",
    "            search_space = {\n",
    "                'kernel': ['poly', 'rbf', 'sigmoid'], \n",
    "                'gamma': ['scale', 'auto'], \n",
    "                'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "        elif ml_methed == 'MLP':\n",
    "            from sklearn.neural_network import MLPClassifier\n",
    "            sk_model = MLPClassifier(random_state=rng, max_iter=500, early_stopping=True)\n",
    "            search_space = {\n",
    "                'hidden_layer_sizes': [(128,), (128, 128), (128, 128, 128)], \n",
    "                'activation': ['logistic', 'tanh', 'relu'], \n",
    "                'solver': ['sgd', 'adam'],\n",
    "                'alpha': [0.1, 0.01, 0.001, 0.0001]}\n",
    "        \n",
    "        elif ml_methed == 'XGBoost':\n",
    "            from sklearn.ensemble import GradientBoostingClassifier\n",
    "            sk_model = GradientBoostingClassifier(n_estimators=100, random_state=rng)\n",
    "            search_space = {\n",
    "                'n_estimators': [50, 100, 250, 500],\n",
    "                'loss': [\"log_loss\", \"exponential\"],\n",
    "                'max_depth': [1, 3, 5],\n",
    "                'learning_rate': [0.01, 0.1, 1],\n",
    "                'min_samples_leaf': [1, 5, 10, 25, 50],\n",
    "                'min_samples_split': [2, 5, 8, 10],\n",
    "                'max_features': ['sqrt', 'log2', None]}\n",
    "        \n",
    "        elif ml_methed == 'KNN':\n",
    "            from sklearn.neighbors import KNeighborsClassifier\n",
    "            sk_model = KNeighborsClassifier(n_neighbors=knnk, n_jobs=n_jobs)\n",
    "            search_space = {'n_neighbors': [1, 3, 5, 10]}\n",
    "\n",
    "    else:\n",
    "        print(f\"\\tError! ML model type should be one of <regression> or <classification>\" )\n",
    "    return sk_model, search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #\n",
    "    modelType=\"regression\"\n",
    "    m_rf = _str_2_bool(args.model_rf)\n",
    "    m_li = _str_2_bool(args.model_linear)\n",
    "    m_svm = _str_2_bool(args.model_svm)\n",
    "    m_mlp = _str_2_bool(args.model_mlp)\n",
    "    m_knn = _str_2_bool(args.model_knn)\n",
    "    m_knnk = int(args.model_knnk)\n",
    "    m_xgb = _str_2_bool(args.model_xgb)\n",
    "    HPT = _str_2_bool(args.HPT)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "########################## Tools ###################################\n",
    "####################################################################\n",
    "## get the args\n",
    "def Args_Prepation(parser_desc):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=parser_desc)\n",
    "    \n",
    "    parser.add_argument('-i', '--input', action=\"store\", default=None, help='The input csv file')\n",
    "    parser.add_argument('-d', '--delimiter', action=\"store\", default=',', help='The delimiter of input csv file for separate columns')\n",
    "    # parser.add_argument('--detectEncoding', action=\"store_true\", help='detect the encoding type of the csv file')\n",
    "    parser.add_argument('--colId', action=\"store\", default='Compound Name', help='The column name of the compound identifier')\n",
    "    parser.add_argument('--colSmi', action=\"store\", default='Structure', help='The column name of the compound smiles')\n",
    "    parser.add_argument('--colPreCalcDesc', action=\"store\", default=None, help='comma separated string e.g., <desc_1,desc_2,desc_3>')   \n",
    "\n",
    "    parser.add_argument('--desc_fps', action=\"store\", default=\"True\", help='calculate the molecular fingerprints')\n",
    "    parser.add_argument('--desc_rdkit', action=\"store\", default=\"True\", help='calculate the molecular property using RDKit')\n",
    "    parser.add_argument('--desc_cx', action=\"store\", default=\"True\", help='calculate the molecular property using ChemAxon')\n",
    "\n",
    "    parser.add_argument('--norm', action=\"store\", default=\"True\", help='normalize the descriptors (z-score)')\n",
    "    parser.add_argument('--imput', action=\"store\", default=\"True\", help='impute the descriptors')\n",
    "    parser.add_argument('-o', '--output', action=\"store\", default=\"./results\", help='the output folder')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
